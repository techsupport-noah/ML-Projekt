{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe-F-FilmDialoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeines\n",
    "\n",
    "Eine allgemeine Beschreibung der Laboraufgaben inklusive des Vorgehens, den Bewertungsrichtlinien und der Abgabe finden Sie  <a href=\"ML-allgemein.ipynb\">hier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenquelle\n",
    "\n",
    "\n",
    "* Laden Sie ihre Daten von http://141.72.190.207/ml_lab/F_dialoge herunter\n",
    "    * Die Daten sind geschützt. \n",
    "        * Sie müssen evtl. in einem Netzwerk der DHBW (z.B. WLAN, VPN, ...) angemeldet sein. \n",
    "        * Sie können sich auf der Webseite mit dem Benutzernamen dhbw und dem Zugangsnamen \"ml_LaB_4$\" anmelden. \n",
    "* Die Daten sind in einem anwendungsspezifischen Format gespeichert.\n",
    "    * Sie finden evtl. Informationen über die Daten in einer \"README\" Datei. \n",
    "    * Finden Sie keine solche Datei sind die Daten selbst erklärend. \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten-Sammlung \n",
    "* besteht aus Dialogen aus verschiedensten Filmen\n",
    "* ist in der Readme Datei beschrieben\n",
    "\n",
    "Erstellen Sie aus den einen Chatbot, der auf eine Frage mit einer Antwort im \"Filmjargon\" antwortet! \n",
    "* Verwenden Sie tiefe Neuronale Netze zu Erstellen des Chatbots! \n",
    "* Passen Sie den Chatbot so an, dass er für unterschiedliche Film-Genres unterschiedlich antwortet! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösung\n",
    "\n",
    "Die Lösung der Aufgabe besteht aus mehreren Teilschritten, welche im Folgenden kurz genannt werden:\n",
    "\n",
    "* Daten einlesen\n",
    "* Daten vorverarbeiten\n",
    "* Model erstellen\n",
    "* Model trainieren\n",
    "* Model abspeichern\n",
    "* Model ausführen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhängigkeiten des Projekts\n",
    "\n",
    "* Tensorflow 2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random \n",
    "import re #re = regular expressions\n",
    "import os\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Abhängigkeiten des Projekts\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_helper as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "Hier werden zuerst die rohen Daten eingelesen und innerhalb der Hilfsfunktion \"readDataToLines\" mit des \"newline\" Zeichen getrennt. Die Daten werden zwei Listen gespeichert, welche dann zurückgegeben werden.\n",
    "Verwendet wurden folgende Funktionen:\n",
    "\n",
    "* \"open\" um die Datei zu öffnen\n",
    "* \"read\" um die Datei zu lesen\n",
    "* \"split\" um die Daten anhand des \"newline\" Zeichen zu trennen und in einer Liste zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the files\n",
    "movie_lines, movie_conversations = dh.readDataToLines(\"data/unzipped/movie_lines.txt\", \"data/unzipped/movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 221616\n",
      "Länge Antworten: 221616\n"
     ]
    }
   ],
   "source": [
    "# extract and mix conversations to a list\n",
    "conversations_list = dh.readConversationsToList(movie_conversations)\n",
    "random.shuffle(conversations_list)\n",
    "\n",
    "# create a dictionary that maps each line id to the corresponding line\n",
    "id2line = dh.readLinesToDict(movie_lines)\n",
    "\n",
    "# remove all unnecessary characters from the lines and replace short forms with the full words\n",
    "id2line = dh.cleanLines(id2line)\n",
    "\n",
    "# split the conversations into requests and responses, each answer is used as a request for the next answer\n",
    "requests, responses = dh.splitConversationsToRequestAndResponse(conversations_list, id2line)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")\n",
    "\n",
    "# delete temporary variables\n",
    "del(movie_lines, movie_conversations, conversations_list, id2line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# get the length of each line of the requests and save the occurrences in a dictionary\n",
    "lengths_request = {}\n",
    "for sentence in requests:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_request:\n",
    "        lengths_request[length] += 1\n",
    "    else:\n",
    "        lengths_request[length] = 1\n",
    "\n",
    "# get the length of each line of the responses and save the occurrences in a dictionary\n",
    "lengths_response = {}\n",
    "for sentence in responses:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_response:\n",
    "        lengths_response[length] += 1\n",
    "    else:\n",
    "        lengths_response[length] = 1\n",
    "\n",
    "# plot the occurrences of the lengths of the requests and responses in the same plot while only showind x values up to 200\n",
    "plt.bar(lengths_request.keys(), lengths_request.values(), label=\"request\")\n",
    "plt.bar(lengths_response.keys(), lengths_response.values(), label=\"response\")\n",
    "plt.xlim(0, 400)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 196695\n",
      "Länge Antworten: 196695\n"
     ]
    }
   ],
   "source": [
    "# based on the result set the parameters\n",
    "max_wordcount_in_sentence = 30\n",
    "\n",
    "# delete requests/responses with length > max_wordcount_in_sentence\n",
    "requests, responses = dh.removeLongSequences(requests, responses, 1, max_wordcount_in_sentence)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the size of the dataset\n",
    "requests = requests[:num_examples]\n",
    "responses = responses[:num_examples]\n",
    "\n",
    "# encapsule the requests and responses with the tokens <S> (Start) and <E> (End)\n",
    "requests = dh.encapsuleWithTokens(requests, '<S>', '<E>')\n",
    "responses = dh.encapsuleWithTokens(responses, 'S', '<E>')\n",
    "\n",
    "\n",
    "# get a dictionary of all unique words with their frequency\n",
    "word2count = dh.getWord2Count(requests, responses)\n",
    "\n",
    "# filter out words with a frequency of 5 or less\n",
    "min_wordFrequency = 20\n",
    "word2count = {k:v for k,v in word2count.items() if v > min_wordFrequency}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man in der obigen Grafik erkennt, sind ab einer Sequenzlänge von 200 nur noch wenige Daten vorhanden. Da die Sequenzlänge maßgeblich auch für den Speicherbedarf in weiteren Schritten ist, wird diese hier auf 200 begrenzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Größe des Wörterbuchs:  19807\n"
     ]
    }
   ],
   "source": [
    "oov_token = '<UNK>'\n",
    "maxlen = len(word2count) + 1\n",
    "del(word2count)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=maxlen, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(requests)\n",
    "tokenizer.fit_on_texts(responses)\n",
    "\n",
    "word2index = tokenizer.word_index\n",
    "index2word ={k:v for v,k in word2index.items()}\n",
    "print('Größe des Wörterbuchs: ',len(word2index))\n",
    "\n",
    "np.save(arr=word2index, file=\"data/word2index.dict\")\n",
    "np.save(arr=index2word, file=\"data/index2word.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tokenizer.texts_to_sequences(requests)\n",
    "target_tensor = tokenizer.texts_to_sequences(responses)\n",
    "\n",
    "max_sen_length = dh.get_maximum_sentence_length(input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "input_tensor = pad_sequences(input_tensor, padding=pad_type, truncating=trunc_type, maxlen=max_sen_length)\n",
    "target_tensor = pad_sequences(target_tensor, padding=pad_type, truncating=trunc_type, maxlen=max_sen_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder_train, input_encoder_val, input_decoder_train, input_decoder_val = train_test_split(input_tensor, target_tensor, test_size=test_size)\n",
    "\n",
    "# Decoder Output erstellen\n",
    "output_decoder_train = [i[1:] for i in input_decoder_train]\n",
    "output_decoder_val = [i[1:] for i in input_decoder_val]\n",
    "\n",
    "#Dimension erhöhen\n",
    "output_decoder_train = pad_sequences(output_decoder_train, padding=pad_type, truncating=trunc_type, maxlen=max_sen_length)\n",
    "output_decoder_val = pad_sequences(output_decoder_val, padding=pad_type, truncating=trunc_type, maxlen=max_sen_length)\n",
    "\n",
    "# convert the decoder input to a one-hot encoded vector which the model can use\n",
    "output_decoder_train = tf.keras.utils.to_categorical(output_decoder_train, num_classes=len(word2index), dtype=\"float32\") # using float32 even though it's memory intensive because later steps need it to be float32\n",
    "output_decoder_val = tf.keras.utils.to_categorical(output_decoder_val, num_classes=len(word2index), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model erstellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDimension = 50 \n",
    "lstm_units = 256\n",
    "max_sentence_length = max_sen_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the encoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputEncoderTensor = tf.keras.Input(shape=(max_sentence_length, ), name = \"inputEncoderTensor\")\n",
    "\n",
    "# embedding layer of the encoder, the input is the input tensor, the output is the embedding tensor\n",
    "encoderEmbedding = tf.keras.layers.Embedding(len(word2index) + 1, output_dim = outputDimension, input_length = max_sentence_length, trainable = True, name = \"embeddingEncoderLayer\")(inputEncoderTensor)\n",
    "\n",
    "# LSTM layer of the encoder, the input is the embedding tensor, the output is the output tensor and the hidden state of the encoder\n",
    "encoderLSTM, encoderHiddenState, encoderCellState = tf.keras.layers.LSTM(units = lstm_units, return_sequences = True, return_state = True, name = \"lstmEncoderLayer\")(encoderEmbedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM\n",
    "- Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the decoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputDecoderTensor = tf.keras.Input(shape=(max_sentence_length, ), name=\"inputDecoderTensor\")\n",
    "\n",
    "# embedding layer of the decoder, the input is the input tensor, the output is the embedding tensor\n",
    "decoderEmbedding = tf.keras.layers.Embedding(len(word2index) + 1, output_dim = outputDimension, input_length = max_sentence_length, trainable = True, name = \"embeddingDecoderLayer\")(inputDecoderTensor)\n",
    "\n",
    "# LSTM layer of the decoder, the input is the embedding tensor and the state of the previous lstm layer, the output is the output tensor and the hidden state of the decoder\n",
    "decoderLSTM, decoderHiddenState, decoderCellState = tf.keras.layers.LSTM(units = lstm_units, return_sequences = True, return_state = True, name = \"lstmDecoderLayer\")(decoderEmbedding, initial_state = [encoderHiddenState, encoderCellState])\n",
    "\n",
    "# dense layer of the decoder, the input is the output tensor of the lstm layer, the output is the output tensor of the dense layer\n",
    "# the dense layer has the same number of units as the number of words in the dictionary because the output of the dense layer is a vector with a probability for each word in the dictionary\n",
    "decoderDense = tf.keras.layers.Dense(units = len(word2index), activation = \"softmax\", name = \"denseLayer\")(decoderLSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputEncoderTensor (InputLayer  [(None, 32)]        0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " inputDecoderTensor (InputLayer  [(None, 32)]        0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " embeddingEncoderLayer (Embeddi  (None, 32, 50)      990400      ['inputEncoderTensor[0][0]']     \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " embeddingDecoderLayer (Embeddi  (None, 32, 50)      990400      ['inputDecoderTensor[0][0]']     \n",
      " ng)                                                                                              \n",
      "                                                                                                  \n",
      " lstmEncoderLayer (LSTM)        [(None, 32, 256),    314368      ['embeddingEncoderLayer[0][0]']  \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstmDecoderLayer (LSTM)        [(None, 32, 256),    314368      ['embeddingDecoderLayer[0][0]',  \n",
      "                                 (None, 256),                     'lstmEncoderLayer[0][1]',       \n",
      "                                 (None, 256)]                     'lstmEncoderLayer[0][2]']       \n",
      "                                                                                                  \n",
      " denseLayer (Dense)             (None, 32, 19807)    5090399     ['lstmDecoderLayer[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,699,935\n",
      "Trainable params: 7,699,935\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model \n",
    "model = tf.keras.models.Model([inputEncoderTensor, inputDecoderTensor], decoderDense)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 241s 314ms/step - loss: 1.8094\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 278s 370ms/step - loss: 1.4236\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 307s 409ms/step - loss: 1.3358\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 264s 351ms/step - loss: 1.2913\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 282s 376ms/step - loss: 1.2631\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 277s 369ms/step - loss: 1.2481\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 267s 356ms/step - loss: 1.2238\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 267s 356ms/step - loss: 1.2084\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 272s 363ms/step - loss: 1.1948\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 271s 361ms/step - loss: 1.1824\n",
      "188/188 [==============================] - 54s 263ms/step - loss: 1.2204\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39m[input_encoder_train, input_decoder_train],y\u001b[39m=\u001b[39moutput_decoder_train , epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, callbacks\u001b[39m=\u001b[39m[earlyStopping])\n\u001b[0;32m      9\u001b[0m \u001b[39m# Evaluate the model on the validation set and store the results\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate([input_encoder_val, input_decoder_val], output_decoder_val)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest loss:\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest accuracy:\u001b[39m\u001b[39m'\u001b[39m, accuracy)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "# generate an early stopping callback to stop training when the validation loss stops improving so that the model doesn't overfit\n",
    "# waits 3 epochs before stopping\n",
    "# use val_loss as the metric because categorical_crossentropy calculates the difference between the predicted and actual values and by monitoring wether or not the loss would be decreasing or increasing we can see if the model is improving or not\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Train the model on the training data and evaluate it on the validation data\n",
    "model.fit(x=[input_encoder_train, input_decoder_train],y=output_decoder_train , epochs=10, batch_size=32, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 41s 218ms/step - loss: 1.2204\n",
      "Test loss: 1.2204091548919678\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and store the results\n",
    "\n",
    "loss = model.evaluate([input_encoder_val, input_decoder_val], output_decoder_val)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noah\\anaconda3\\envs\\tensorflow_ml_stuff\\lib\\site-packages\\numpy\\lib\\npyio.py:716: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "# Modell speichern\n",
    "dh.save_model(model, 'chatbot.h5')\n",
    "\n",
    "# Gewichte speichern\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights != []:\n",
    "        np.savez(f'models/{layer.name}.npz', weights, dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 32, 50)       990400      ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 32, 256),    314368      ['embedding[0][0]']              \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 32, 256),    314368      ['embedding[1][0]',              \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32, 19807)    5090399     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,709,535\n",
      "Trainable params: 6,709,535\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"lstm_1\" (type LSTM).\n\nDimensions must be equal, but are 400 and 256 for '{{node MatMul_1}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](init_h, recurrent_kernel)' with input shapes: [?,400], [256,1024].\n\nCall arguments received by layer \"lstm_1\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, 32, 50), dtype=float32)', 'tf.Tensor(shape=(None, 400), dtype=float32)', 'tf.Tensor(shape=(None, 400), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m decoder_state_input_c_placeholder \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m400\u001b[39m, ))\n\u001b[0;32m     66\u001b[0m decoder_state_inputs_placeholder \u001b[39m=\u001b[39m [decoder_state_input_h_placeholder, decoder_state_input_c_placeholder]\n\u001b[1;32m---> 67\u001b[0m decoder_ouputs, state_h, state_c \u001b[39m=\u001b[39m decoder_lstm(decoder_embedding, initial_state\u001b[39m=\u001b[39;49mdecoder_state_inputs_placeholder)\n\u001b[0;32m     68\u001b[0m decoder_states \u001b[39m=\u001b[39m [state_h, state_c]\n\u001b[0;32m     69\u001b[0m \u001b[39m# Decoder-Modell erstellen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noah\\anaconda3\\envs\\tensorflow_ml_stuff\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:612\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[39m# Perform the call with temporarily replaced input_spec\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m full_input_spec\n\u001b[1;32m--> 612\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(full_input, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    613\u001b[0m \u001b[39m# Remove the additional_specs from input spec and keep the rest. It\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[39m# is important to keep since the input spec was populated by\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[39m# build(), and will be reused in the stateful=True.\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(additional_specs)]\n",
      "File \u001b[1;32mc:\\Users\\noah\\anaconda3\\envs\\tensorflow_ml_stuff\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\noah\\anaconda3\\envs\\tensorflow_ml_stuff\\lib\\site-packages\\keras\\backend.py:2455\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   2453\u001b[0m     out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39msparse_dense_matmul(x, y)\n\u001b[0;32m   2454\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2455\u001b[0m     out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(x, y)\n\u001b[0;32m   2456\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"lstm_1\" (type LSTM).\n\nDimensions must be equal, but are 400 and 256 for '{{node MatMul_1}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](init_h, recurrent_kernel)' with input shapes: [?,400], [256,1024].\n\nCall arguments received by layer \"lstm_1\" (type LSTM):\n  • inputs=['tf.Tensor(shape=(None, 32, 50), dtype=float32)', 'tf.Tensor(shape=(None, 400), dtype=float32)', 'tf.Tensor(shape=(None, 400), dtype=float32)']\n  • mask=None\n  • training=None\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "# Ungetestet muss noch angepasst werden\n",
    "\n",
    "outputDimension = 50 \n",
    "lstm_units = 256\n",
    "max_sentence_length = 32\n",
    "\n",
    "\"\"\"\n",
    "Laden der benötigten Daten\n",
    "\"\"\"\n",
    "# Laden der beiden Vokabulare\n",
    "index2word = np.load('./data/index2word.dict.npy',allow_pickle='TRUE').item()\n",
    "word2index = np.load('./data/word2index.dict.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "# Laden der Layer\n",
    "w_embedding = np.load('./models/embeddingEncoderLayer.npz', allow_pickle=True)\n",
    "w_encoder_lstm = np.load('./models/lstmEncoderLayer.npz', allow_pickle=True)\n",
    "w_decoder_lstm = np.load('./models/lstmDecoderLayer.npz', allow_pickle=True)\n",
    "w_dense = np.load('./models/denseLayer.npz', allow_pickle=True)\n",
    "\n",
    "\"\"\"\n",
    "Erstellen des Trainingsmodelles\n",
    "Genauere Ausführungen sind in dem Hauptdokument zu finden\n",
    "\"\"\"\n",
    "# Eingabe-Platzhalter für Encoder und Decoder erstellen\n",
    "encoder_input_placeholder = Input(shape=(max_sentence_length, ))\n",
    "decoder_input_placeholder = Input(shape=(max_sentence_length, ))\n",
    "# Embedding-Layer erstellen\n",
    "VOCABULARY_SIZE = len(index2word)\n",
    "embedding = Embedding(VOCABULARY_SIZE+1, output_dim=50, input_length=13, trainable=True)\n",
    "# Encoder-Eingabe dem Emebdding-Layer übergeben\n",
    "encoder_embedding = embedding(encoder_input_placeholder)\n",
    "# Encoder-LSTM-Layer erstellen\n",
    "encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "# Encoder-Embedding-Ausgabe dem Encoder-LSTM-Layer übergeben\n",
    "encoder_output, h, c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [h, c]\n",
    "# Decoder-LSTM-Layer erstellen\n",
    "decoder_embedding = embedding(decoder_input_placeholder)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# Erstellen des Dense-Layers\n",
    "dense = Dense(VOCABULARY_SIZE, activation=\"softmax\")\n",
    "# Decoder-Ausgabe dem Dense-Layer übergeben, um Dense-Ausgabe zu erhalten\n",
    "dense_output = dense(decoder_output)\n",
    "loaded_model = Model([encoder_input_placeholder, decoder_input_placeholder], dense_output)\n",
    "\n",
    "loaded_model.summary()\n",
    "\n",
    "\"\"\"\n",
    "Anwenden der geladenen Layer auf das neue Modell\n",
    "\"\"\"\n",
    "loaded_model.layers[2].set_weights(w_embedding['arr_0'])\n",
    "loaded_model.layers[3].set_weights(w_encoder_lstm['arr_0'])\n",
    "loaded_model.layers[4].set_weights(w_decoder_lstm['arr_0'])\n",
    "loaded_model.layers[5].set_weights(w_dense['arr_0'])\n",
    "\n",
    "\"\"\"\n",
    "Erstellen des Interferenz-Modells\n",
    "Genauere Ausführungen sind in dem Hauptdokument zu finden\n",
    "\"\"\"\n",
    "# Encoder-Modell erstellen\n",
    "encoder_model = Model([encoder_input_placeholder], encoder_states)\n",
    "# Decoder Eingaben erstellen\n",
    "decoder_state_input_h_placeholder = Input(shape=(400, ))\n",
    "decoder_state_input_c_placeholder = Input(shape=(400, ))\n",
    "decoder_state_inputs_placeholder = [decoder_state_input_h_placeholder, decoder_state_input_c_placeholder]\n",
    "decoder_ouputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_state_inputs_placeholder)\n",
    "decoder_states = [state_h, state_c]\n",
    "# Decoder-Modell erstellen\n",
    "decoder_model = Model([decoder_input_placeholder] + decoder_state_inputs_placeholder, [decoder_ouputs] + decoder_states)\n",
    "\n",
    "\"\"\"\n",
    "Ausführung des interaktiven Chatbots\n",
    "\"\"\"\n",
    "print(\"##############################\\n#   Chatbot   #\\n##############################\")\n",
    "\n",
    "# Eingabe des Benutzers\n",
    "user_input = input(\"You: \")\n",
    "\n",
    "while user_input != \"quit\":\n",
    "    # Vorbereitung\n",
    "    user_input = clean_text(user_input)\n",
    "    user_input_list = [user_input]\n",
    "\n",
    "    # Eingabe in ganzzahlige Werte konvertieren\n",
    "    text = []\n",
    "    for input_list in user_input_list:\n",
    "        word_list = []\n",
    "        for word in input_list.split():\n",
    "            try:\n",
    "                word_list.append(vocabulary[word])\n",
    "            except:\n",
    "                word_list.append(vocabulary[\"<OUT>\"])\n",
    "        text.append(word_list)\n",
    "\n",
    "    # Länge 13 festlegen\n",
    "    text = pad_sequences(text, 13, padding=\"post\")\n",
    "\n",
    "    states = encoder_model.predict(text)\n",
    "\n",
    "    empty_target_sequence = np.zeros((1, 1))\n",
    "    empty_target_sequence[0,0] = vocabulary[\"<START>\"]\n",
    "\n",
    "    # Dauerschleife bis Ende des Strings oder maximale Länge erreicht\n",
    "    stop_condition = False\n",
    "    answer = \"\"\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        decoder_ouputs, h, c = decoder_model.predict([empty_target_sequence] + states)\n",
    "\n",
    "        # Dense mit Softmax-Aktivierung\n",
    "        decoder_concatination_input = dense(decoder_ouputs)\n",
    "\n",
    "        # Index des Wortes mit der höchsten Wahrscheinlichkeit\n",
    "        sampled_word_index = np.argmax(decoder_concatination_input[0, -1, :])\n",
    "\n",
    "        # Wort über invertiertes Vokabular finden\n",
    "        sampled_word = inverted_vocabulary[sampled_word_index] + \" \"\n",
    "\n",
    "        # Wort als Chatbot-Antwort speichern\n",
    "        if sampled_word != \"<END> \":\n",
    "            answer += sampled_word\n",
    "\n",
    "        # Antwort beenden falls <END>-Token oder maximale Länge erreicht\n",
    "        if sampled_word == \"<END> \" or len(answer.split()) > 13:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Werte zurücksetzen\n",
    "        empty_target_sequence = np.zeros((1, 1))\n",
    "        empty_target_sequence[0, 0] = sampled_word_index\n",
    "        states = [h, c]\n",
    "\n",
    "    print(\"Chatbot: \", answer, \"\\n==============================\")\n",
    "    # Hier nächste Eingabe des Benutzers, um die Abbruchbedingung reichtzeiH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9978dee63576bf3eb01fd0099124cb1b0d7ef3663923ee832c1b14872d283213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
