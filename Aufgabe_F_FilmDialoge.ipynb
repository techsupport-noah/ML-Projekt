{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe-F-FilmDialoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeines\n",
    "\n",
    "Eine allgemeine Beschreibung der Laboraufgaben inklusive des Vorgehens, den Bewertungsrichtlinien und der Abgabe finden Sie  <a href=\"ML-allgemein.ipynb\">hier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenquelle\n",
    "\n",
    "\n",
    "* Laden Sie ihre Daten von http://141.72.190.207/ml_lab/F_dialoge herunter\n",
    "    * Die Daten sind geschützt. \n",
    "        * Sie müssen evtl. in einem Netzwerk der DHBW (z.B. WLAN, VPN, ...) angemeldet sein. \n",
    "        * Sie können sich auf der Webseite mit dem Benutzernamen dhbw und dem Zugangsnamen \"ml_LaB_4$\" anmelden. \n",
    "* Die Daten sind in einem anwendungsspezifischen Format gespeichert.\n",
    "    * Sie finden evtl. Informationen über die Daten in einer \"README\" Datei. \n",
    "    * Finden Sie keine solche Datei sind die Daten selbst erklärend. \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten-Sammlung \n",
    "* besteht aus Dialogen aus verschiedensten Filmen\n",
    "* ist in der Readme Datei beschrieben\n",
    "\n",
    "Erstellen Sie aus den einen Chatbot, der auf eine Frage mit einer Antwort im \"Filmjargon\" antwortet! \n",
    "* Verwenden Sie tiefe Neuronale Netze zu Erstellen des Chatbots! \n",
    "* Passen Sie den Chatbot so an, dass er für unterschiedliche Film-Genres unterschiedlich antwortet! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösung\n",
    "\n",
    "Die Lösung der Aufgabe besteht aus mehreren Teilschritten, welche im Folgenden kurz genannt werden:\n",
    "\n",
    "* Daten einlesen\n",
    "* Daten vorverarbeiten\n",
    "* Model erstellen\n",
    "* Model trainieren\n",
    "* Model abspeichern\n",
    "* Interferenzmodel erstellen\n",
    "* Interferenzmodel ausführen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhängigkeiten des Projekts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Abhängigkeiten des Projekts\n",
    "\n",
    "* hier werden unsere Hilfsfunktionen importiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_helper as dh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstanten\n",
    "Wähle hier u.a. welches Genre genutzt werden soll. Mit dem Flag \"all\" werden alle Daten verwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = \"all\"\n",
    "num_examples = 30000\n",
    "test_size = 0.2\n",
    "random.seed(42)\n",
    "max_wordcount_in_sentence = 10\n",
    "max_sentence_length = max_wordcount_in_sentence + 2 # +2 for start and end token in decoder data\n",
    "outputDimension = 50\n",
    "lstm_units = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "Hier werden zuerst die rohen Daten eingelesen und innerhalb der Hilfsfunktion \"readDataToLines\" mit des \"newline\" Zeichen getrennt. Die Daten werden zwei Listen gespeichert, welche dann zurückgegeben werden.\n",
    "Verwendet wurden folgende Funktionen:\n",
    "\n",
    "* \"open\" um die Datei zu öffnen\n",
    "* \"read\" um die Datei zu lesen\n",
    "* \"split\" um die Daten anhand des \"newline\" Zeichen zu trennen und in einer Liste zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the files\n",
    "movie_lines, movie_conversations, movie_data = dh.readDataToLines(\"data/unzipped/movie_lines.txt\", \"data/unzipped/movie_conversations.txt\", \"data/unzipped/movie_titles_metadata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorverarbeiten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden die Daten vorverarbeitet.\n",
    "\n",
    "Dazu werden die Konversationen aus der conversations-Datei, bestehend aus Tokens, in einer Liste gespeichert.\n",
    "Anschließend werden, um die Verallgemeinerung des Modells zu verbessern, die Konversationen zufällig (mit Seed) gemischt.\n",
    "Mit Hilfe der lines-Datei, bestehend aus Tokens und zugewiesenen Zeilen, wird ein Dictionary erstellt, welches die Zeilen mit den Tokens verknüpft.\n",
    "Anschließend werden alle Zeilen mit Hilfe einer Hilfsfunktion von unnötigen Zeichen bereinigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary that maps each line id to the corresponding line\n",
    "id2line = dh.readLinesToDict(movie_lines)\n",
    "\n",
    "# remove all unnecessary characters from the lines and replace short forms with the full words\n",
    "id2line = dh.cleanLines(id2line)\n",
    "\n",
    "if genre == \"all\":\n",
    "    # extract and mix conversations to a list\n",
    "    conversations_list = dh.readConversationsToList(movie_conversations)\n",
    "else:\n",
    "    id2genre = dh.readMoviedataToDict(movie_data)\n",
    "    conversations_list_per_genre = dh.readConversationsToListDependingGenre(movie_conversations, id2genre)\n",
    "    assert genre in conversations_list_per_genre.keys()\n",
    "    conversations_list = conversations_list_per_genre[genre]\n",
    "\n",
    "random.shuffle(conversations_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden die Konversationen in Requests und Responses unterteilt.\n",
    "Konversationen mit mehr als 2 Zeilen, werden dabei in x Konversationen mit jeweils 2 Zeilen unterteilt, weil jede Antwort auch selbst wieder ein Request für die nächste Antwort ist.\n",
    "Zur Kontrolle wird die Länge beider Datensätze ausgegeben. Man erkennt, dass beide Listen gleich lang sind, wir also zu jedem Request einen passenden Response haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 221616\n",
      "Länge Antworten: 221616\n"
     ]
    }
   ],
   "source": [
    "# split the conversations into requests and responses, each answer is used as a request for the next answer\n",
    "requests, responses = dh.splitConversationsToRequestAndResponse(conversations_list, id2line)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter wählen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu bestimmen, mit welchen Parametern wir die Daten eingrenzen wollen, da die Datenmenge sehr groß ist, werden die Längen der Requests und Responses ausgegeben und deren Häufigkeit grafisch visualisiert.\n",
    "Man erkennt gut, dass die meisten Daten eine Wortanzahl bis zu 10 Worten haben. Deshalb wurden die Daten auf diese Länge beschränkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the length of each line of the requests and save the occurrences in a dictionary\n",
    "lengths_request = {}\n",
    "for sentence in requests:\n",
    "    length = len(sentence.split())\n",
    "    if length in lengths_request:\n",
    "        lengths_request[length] += 1\n",
    "    else:\n",
    "        lengths_request[length] = 1\n",
    "\n",
    "# get the length of each line of the responses and save the occurrences in a dictionary\n",
    "lengths_response = {}\n",
    "for sentence in responses:\n",
    "    length = len(sentence.split())\n",
    "    if length in lengths_response:\n",
    "        lengths_response[length] += 1\n",
    "    else:\n",
    "        lengths_response[length] = 1\n",
    "\n",
    "# plot the occurrences of the lengths of the requests and responses in the same plot while only showind x values up to 200\n",
    "plt.bar(lengths_request.keys(), lengths_request.values(), label=\"request\")\n",
    "plt.bar(lengths_response.keys(), lengths_response.values(), label=\"response\")\n",
    "plt.xlim(0, 100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden nun die Daten um die Datenpaare gekürzt, die die gewählte Länge überschreiten.\n",
    "Zur Überprüfung wird erneut die Länge der beiden Datensätze ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 97310\n",
      "Länge Antworten: 97310\n"
     ]
    }
   ],
   "source": [
    "# delete requests and associated responses with length > max_wordcount_in_sentence\n",
    "requests, responses = dh.removeLongSequences(requests, responses, 1, max_wordcount_in_sentence)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Datensatz noch weiter zu reduzieren, werden die Datensätze basierend auf einer festen Länge gekürzt. Im konkreten Fall konnten wir mit Datensatzgrößen von maximal jeweils ca. 30000 Daten arbeiten, da die Datenmenge sonst nicht in den Arbeitsspeicher passt. \n",
    "Außerdem wurden die Daten, welche im Decoder zum Training verwendet werden sollen um Start und End Tokens ergänzt. Dadurch wird der Decoder angewiesen, dass er mit dem Start Token beginnen soll und mit dem End Token aufhören soll.\n",
    "Basierend auf diesem Tokens können wir das Generieren der Antworten starten und das Predicten ab dem Stop Token beenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the size of the dataset\n",
    "requests = requests[:num_examples]\n",
    "responses = responses[:num_examples]\n",
    "\n",
    "# encapsule the responses with the tokens <S> (Start) and <E> (End)\n",
    "responses = dh.encapsuleWithTokens(responses, '<S>', '<E>')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden das jeweilige Vorkommen der einzelnen Wörter in Anfragen und Antworten gezählt. Wörter die weniger als 5 mal vorkommen werden dabei herausgefiltert. Schließlich wird die Anzahl der übrigen Wörter bestimmt. +2 um die zusätzlichen Tokens *U* und *P* zu berücksichtigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl an Wörtern: 5521\n"
     ]
    }
   ],
   "source": [
    "word2count = dh.getWord2Count(requests, responses)\n",
    "\n",
    "min_wordFrequency = 3\n",
    "word2count = {k:v for k,v in word2count.items() if v >= min_wordFrequency}\n",
    "vocab_size = len(word2count)+2\n",
    "print(f\"Anzahl an Wörtern: {vocab_size}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Hilfe des Tokenizers können wir nun die vorkommenden Wörter in den Datensätzen mit einem eindeutigen Index versehen und die Datensätze in Sequenzen von Indexen umwandeln. Hierbei bestimmt die Vocab_size die maximal vergebenen Tokens. Hierdurch werden die Wörter mit dem niedrigsten Vorkommen durch den Token *U* (Unknown) ersetzt.\n",
    "Zur Kontrolle werden die ersten 5 Daten innerhalb der Datensätze vor und nach der Tokenisierung ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my room', 'mine too', 'get out of here', 'would you like part of this it is not much', 'like you']\n",
      "['<S> mine too <E>', '<S> what were they looking for <E>', '<S> it is really that bad <E>', '<S> all the insides are gone <E>', '<S> beautiful <E>']\n",
      "[[29, 267], [376, 91], [53, 65, 23, 41], [60, 4, 43, 548, 23, 24, 8, 6, 17, 118], [43, 4]]\n",
      "[[2, 376, 91, 3], [2, 9, 78, 45, 226, 31, 3], [2, 8, 6, 90, 13, 188, 3], [2, 56, 7, 1, 12, 320, 3], [2, 290, 3]]\n"
     ]
    }
   ],
   "source": [
    "oov_token = '<U>'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token, filters='', lower=False)\n",
    "\n",
    "tokenizer.fit_on_texts(requests)\n",
    "tokenizer.fit_on_texts(responses)\n",
    "tokenizer.fit_on_texts([\"<P>\"]) # add padding token to the tokenizer\n",
    "\n",
    "tokenized_requests = tokenizer.texts_to_sequences(requests)\n",
    "tokenized_responses = tokenizer.texts_to_sequences(responses)\n",
    "\n",
    "print(requests[0:5])\n",
    "print(responses[0:5])\n",
    "print(tokenized_requests[0:5])\n",
    "print(tokenized_responses[0:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um alle Daten auf eine einheitliche Länge zu bringen, werden die Datensätze mit Padding aufgefüllt.\n",
    "Um die Daten korrekt mit Padding Tokens zu füllen, wird der Index des Padding Tokens ermittelt um ihn anschließend beim Padding zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_index = tokenizer.word_index[\"<P>\"]\n",
    "\n",
    "pad_type = 'post'\n",
    "trunc_type = 'post'\n",
    "\n",
    "encoder_input = pad_sequences(tokenized_requests, padding=pad_type, maxlen=max_sentence_length, truncating=trunc_type, value=padding_index)\n",
    "decoder_input = pad_sequences(tokenized_responses, padding=pad_type, maxlen=max_sentence_length, truncating=trunc_type, value=padding_index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weil dem Decoder Ausgaben das Start Token vorangestellt wird, muss das Start Token beim letztendlichen Output der Trainingsdaten des Decoders entfernt werden.\n",
    "Anschließend wird die Dimension des Outputs erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_encoder_train, input_encoder_val, input_decoder_train, input_decoder_val = train_test_split(encoder_input, decoder_input, test_size=test_size)\n",
    "\n",
    "output_decoder_train = [i[1:] for i in input_decoder_train]\n",
    "output_decoder_train = pad_sequences(output_decoder_train, maxlen=max_sentence_length, padding='post', truncating=\"post\", value=padding_index)\n",
    "output_decoder_train = tf.keras.utils.to_categorical(output_decoder_train, vocab_size)\n",
    "\n",
    "output_decoder_val = [i[1:] for i in input_decoder_val]\n",
    "output_decoder_val = pad_sequences(output_decoder_val, maxlen=max_sentence_length, padding='post', truncating=\"post\", value=padding_index)\n",
    "output_decoder_val = tf.keras.utils.to_categorical(output_decoder_val, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model erstellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Parameter des Models wurden im Bereich der Konstanten definiert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Shared Embedding Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Shared Embedding Layer erstellt. Dieser Layer wird sowohl vom Encoder als auch vom Decoder Teil verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(vocab_size, output_dim = outputDimension, trainable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Encoder Teil des Models erstellt. Er besteht aus einem Input Layer, einem Embedding Layer und einem LSTM Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the encoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputEncoderTensor = tf.keras.Input(shape=(None, ))\n",
    "\n",
    "# embedding layer of the encoder, the input is the input tensor, the output is the embedding tensor\n",
    "encoderEmbedding = embedding(inputEncoderTensor)\n",
    "\n",
    "# LSTM layer of the encoder, the input is the embedding tensor, the output is the output tensor and the hidden state of the encoder\n",
    "encoderLSTM = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state = True)\n",
    "encoderOutput, encoderHiddenState, encoderCellState = encoderLSTM(encoderEmbedding)\n",
    "encoderStates = [encoderHiddenState, encoderCellState]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM\n",
    "- Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Decoder Teil des Models erstellt. Er besteht aus einem Input Layer, einem Embedding Layer, einem LSTM Layer und einem Dense Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the decoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputDecoderTensor = tf.keras.Input(shape=(None, ))\n",
    "\n",
    "# embedding layer of the decoder, the input is the input tensor, the output is the embedding tensor\n",
    "decoderEmbedding = embedding(inputDecoderTensor)\n",
    "\n",
    "# LSTM layer of the decoder, the input is the embedding tensor and the state of the previous lstm layer, the output is the output tensor and the hidden state of the decoder\n",
    "decoderLSTM = tf.keras.layers.LSTM(lstm_units, return_state = True, return_sequences=True)\n",
    "decoderOutput, _, _ = decoderLSTM(decoderEmbedding, initial_state = encoderStates)\n",
    "\n",
    "# dense layer of the decoder, the input is the output tensor of the lstm layer, the output is the output tensor of the dense layer\n",
    "# the dense layer has the same number of units as the number of words in the dictionary because the output of the dense layer is a vector with a probability for each word in the dictionary\n",
    "decoderDense = tf.keras.layers.Dense(vocab_size, activation = \"softmax\")\n",
    "outputDense = decoderDense(decoderOutput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das Trainingsmodel aus beiden Teilen (Encoder, Decoder) erstellt. Für die Überwachung des Trainingsprozesses kommt die Genauigkeit als Metrik zum Einsatz. Weiterhin wird der Optimizer Adam und die Loss Funktion Categorical Crossentropy verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     727950      ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, None, 400),  721600      ['embedding[0][0]']              \n",
      "                                 (None, 400),                                                     \n",
      "                                 (None, 400)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 400),  721600      ['embedding[1][0]',              \n",
      "                                 (None, 400),                     'lstm[0][1]',                   \n",
      "                                 (None, 400)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 14559)  5838159     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,009,309\n",
      "Trainable params: 8,009,309\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if genre == \"all\":\n",
    "    # Define the model \n",
    "    model = tf.keras.models.Model([inputEncoderTensor, inputDecoderTensor], outputDense)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    # load the base model \n",
    "    model = load_model(f\"models/model_all.h5\")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren des Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um eine Überanpassung des Models zu vermeiden, wird ein Callback erstellt, welches das Training abbricht, wenn die Genauigkeit wärend des Trainings 5 Epochen lang nicht mehr gestiegen ist.\n",
    "Anschließend wird das Model 200 Epochen lang trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "750/750 [==============================] - 95s 122ms/step - loss: 2.7841 - accuracy: 0.5843\n",
      "Epoch 2/200\n",
      "750/750 [==============================] - 87s 115ms/step - loss: 2.3564 - accuracy: 0.6233\n",
      "Epoch 3/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 2.2299 - accuracy: 0.6333\n",
      "Epoch 4/200\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 2.1461 - accuracy: 0.6392\n",
      "Epoch 5/200\n",
      "750/750 [==============================] - 87s 116ms/step - loss: 2.0828 - accuracy: 0.6428\n",
      "Epoch 6/200\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 2.0271 - accuracy: 0.6458\n",
      "Epoch 7/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 1.9747 - accuracy: 0.6481\n",
      "Epoch 8/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 1.9235 - accuracy: 0.6504\n",
      "Epoch 9/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 1.8724 - accuracy: 0.6521\n",
      "Epoch 10/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 1.8211 - accuracy: 0.6545\n",
      "Epoch 11/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 1.7714 - accuracy: 0.6572\n",
      "Epoch 12/200\n",
      "750/750 [==============================] - 95s 126ms/step - loss: 1.7206 - accuracy: 0.6605\n",
      "Epoch 13/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 1.6721 - accuracy: 0.6644\n",
      "Epoch 14/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 1.6250 - accuracy: 0.6693\n",
      "Epoch 15/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 1.5803 - accuracy: 0.6745\n",
      "Epoch 16/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 1.5399 - accuracy: 0.6797\n",
      "Epoch 17/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 1.5001 - accuracy: 0.6850\n",
      "Epoch 18/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 1.4629 - accuracy: 0.6904\n",
      "Epoch 19/200\n",
      "750/750 [==============================] - 90s 121ms/step - loss: 1.4278 - accuracy: 0.6956\n",
      "Epoch 20/200\n",
      "750/750 [==============================] - 93s 123ms/step - loss: 1.3946 - accuracy: 0.7008\n",
      "Epoch 21/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 1.3612 - accuracy: 0.7072\n",
      "Epoch 22/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 1.3295 - accuracy: 0.7126\n",
      "Epoch 23/200\n",
      "750/750 [==============================] - 96s 128ms/step - loss: 1.2984 - accuracy: 0.7180\n",
      "Epoch 24/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 1.2684 - accuracy: 0.7235\n",
      "Epoch 25/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 1.2395 - accuracy: 0.7289\n",
      "Epoch 26/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 1.2112 - accuracy: 0.7336\n",
      "Epoch 27/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 1.1830 - accuracy: 0.7397\n",
      "Epoch 28/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 1.1553 - accuracy: 0.7446\n",
      "Epoch 29/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 1.1293 - accuracy: 0.7501\n",
      "Epoch 30/200\n",
      "750/750 [==============================] - 93s 125ms/step - loss: 1.1014 - accuracy: 0.7556\n",
      "Epoch 31/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 1.0755 - accuracy: 0.7611\n",
      "Epoch 32/200\n",
      "750/750 [==============================] - 93s 123ms/step - loss: 1.0501 - accuracy: 0.7661\n",
      "Epoch 33/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 1.0239 - accuracy: 0.7713\n",
      "Epoch 34/200\n",
      "750/750 [==============================] - 93s 125ms/step - loss: 0.9991 - accuracy: 0.7761\n",
      "Epoch 35/200\n",
      "750/750 [==============================] - 93s 125ms/step - loss: 0.9736 - accuracy: 0.7819\n",
      "Epoch 36/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.9488 - accuracy: 0.7873\n",
      "Epoch 37/200\n",
      "750/750 [==============================] - 96s 128ms/step - loss: 0.9242 - accuracy: 0.7921\n",
      "Epoch 38/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.9003 - accuracy: 0.7970\n",
      "Epoch 39/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 0.8752 - accuracy: 0.8022\n",
      "Epoch 40/200\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.8516 - accuracy: 0.8075\n",
      "Epoch 41/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.8279 - accuracy: 0.8126\n",
      "Epoch 42/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.8042 - accuracy: 0.8177\n",
      "Epoch 43/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.7819 - accuracy: 0.8225\n",
      "Epoch 44/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 0.7585 - accuracy: 0.8280\n",
      "Epoch 45/200\n",
      "750/750 [==============================] - 93s 123ms/step - loss: 0.7369 - accuracy: 0.8329\n",
      "Epoch 46/200\n",
      "750/750 [==============================] - 97s 129ms/step - loss: 0.7139 - accuracy: 0.8381\n",
      "Epoch 47/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.6920 - accuracy: 0.8431\n",
      "Epoch 48/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.6717 - accuracy: 0.8474\n",
      "Epoch 49/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 0.6503 - accuracy: 0.8522\n",
      "Epoch 50/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.6298 - accuracy: 0.8572\n",
      "Epoch 51/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.6097 - accuracy: 0.8618\n",
      "Epoch 52/200\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 0.5900 - accuracy: 0.8662\n",
      "Epoch 53/200\n",
      "750/750 [==============================] - 96s 128ms/step - loss: 0.5714 - accuracy: 0.8706\n",
      "Epoch 54/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.5529 - accuracy: 0.8747\n",
      "Epoch 55/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.5346 - accuracy: 0.8787\n",
      "Epoch 56/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 0.5171 - accuracy: 0.8833\n",
      "Epoch 57/200\n",
      "750/750 [==============================] - 93s 125ms/step - loss: 0.4992 - accuracy: 0.8875\n",
      "Epoch 58/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.4829 - accuracy: 0.8912\n",
      "Epoch 59/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 0.4692 - accuracy: 0.8940\n",
      "Epoch 60/200\n",
      "750/750 [==============================] - 90s 121ms/step - loss: 0.4518 - accuracy: 0.8982\n",
      "Epoch 61/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 0.4365 - accuracy: 0.9018\n",
      "Epoch 62/200\n",
      "750/750 [==============================] - 98s 131ms/step - loss: 0.4227 - accuracy: 0.9048\n",
      "Epoch 63/200\n",
      "750/750 [==============================] - 88s 117ms/step - loss: 0.4090 - accuracy: 0.9080\n",
      "Epoch 64/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.3948 - accuracy: 0.9114\n",
      "Epoch 65/200\n",
      "750/750 [==============================] - 98s 130ms/step - loss: 0.3804 - accuracy: 0.9150\n",
      "Epoch 66/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 0.3690 - accuracy: 0.9175\n",
      "Epoch 67/200\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 0.3573 - accuracy: 0.9199\n",
      "Epoch 68/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 0.3475 - accuracy: 0.9221\n",
      "Epoch 69/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.3352 - accuracy: 0.9249\n",
      "Epoch 70/200\n",
      "750/750 [==============================] - 96s 128ms/step - loss: 0.3238 - accuracy: 0.9276\n",
      "Epoch 71/200\n",
      "750/750 [==============================] - 95s 127ms/step - loss: 0.3140 - accuracy: 0.9298\n",
      "Epoch 72/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.3041 - accuracy: 0.9318\n",
      "Epoch 73/200\n",
      "750/750 [==============================] - 95s 126ms/step - loss: 0.2938 - accuracy: 0.9347\n",
      "Epoch 74/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.2861 - accuracy: 0.9364\n",
      "Epoch 75/200\n",
      "750/750 [==============================] - 89s 118ms/step - loss: 0.2781 - accuracy: 0.9377\n",
      "Epoch 76/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.2701 - accuracy: 0.9398\n",
      "Epoch 77/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 0.2617 - accuracy: 0.9415\n",
      "Epoch 78/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.2531 - accuracy: 0.9434\n",
      "Epoch 79/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.2458 - accuracy: 0.9451\n",
      "Epoch 80/200\n",
      "750/750 [==============================] - 88s 117ms/step - loss: 0.2404 - accuracy: 0.9462\n",
      "Epoch 81/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.2370 - accuracy: 0.9470\n",
      "Epoch 82/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.2265 - accuracy: 0.9491\n",
      "Epoch 83/200\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.2189 - accuracy: 0.9512\n",
      "Epoch 84/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 0.2147 - accuracy: 0.9514\n",
      "Epoch 85/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 0.2109 - accuracy: 0.9523\n",
      "Epoch 86/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.2072 - accuracy: 0.9531\n",
      "Epoch 87/200\n",
      "750/750 [==============================] - 88s 118ms/step - loss: 0.1997 - accuracy: 0.9548\n",
      "Epoch 88/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1928 - accuracy: 0.9563\n",
      "Epoch 89/200\n",
      "750/750 [==============================] - 92s 123ms/step - loss: 0.1864 - accuracy: 0.9578\n",
      "Epoch 90/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 0.1841 - accuracy: 0.9582\n",
      "Epoch 91/200\n",
      "750/750 [==============================] - 93s 125ms/step - loss: 0.1831 - accuracy: 0.9582\n",
      "Epoch 92/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.1811 - accuracy: 0.9585\n",
      "Epoch 93/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.1755 - accuracy: 0.9600\n",
      "Epoch 94/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1698 - accuracy: 0.9612\n",
      "Epoch 95/200\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1657 - accuracy: 0.9621\n",
      "Epoch 96/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 0.1597 - accuracy: 0.9634\n",
      "Epoch 97/200\n",
      "750/750 [==============================] - 90s 120ms/step - loss: 0.1604 - accuracy: 0.9632\n",
      "Epoch 98/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.1576 - accuracy: 0.9636\n",
      "Epoch 99/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 0.1565 - accuracy: 0.9638\n",
      "Epoch 100/200\n",
      "750/750 [==============================] - 88s 117ms/step - loss: 0.1548 - accuracy: 0.9640\n",
      "Epoch 101/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.1516 - accuracy: 0.9648\n",
      "Epoch 102/200\n",
      "750/750 [==============================] - 94s 125ms/step - loss: 0.1422 - accuracy: 0.9672\n",
      "Epoch 103/200\n",
      "750/750 [==============================] - 89s 119ms/step - loss: 0.1398 - accuracy: 0.9675\n",
      "Epoch 104/200\n",
      "750/750 [==============================] - 94s 126ms/step - loss: 0.1410 - accuracy: 0.9672\n",
      "Epoch 105/200\n",
      "750/750 [==============================] - 96s 128ms/step - loss: 0.1397 - accuracy: 0.9672\n",
      "Epoch 106/200\n",
      "750/750 [==============================] - 91s 121ms/step - loss: 0.1396 - accuracy: 0.9673\n",
      "Epoch 107/200\n",
      "750/750 [==============================] - 91s 122ms/step - loss: 0.1447 - accuracy: 0.9653\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d98baafac0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate an early stopping callback to stop training when the validation loss stops improving so that the model doesn't overfit\n",
    "# waits 3 epochs before stopping\n",
    "# use val_loss as the metric because categorical_crossentropy calculates the difference between the predicted and actual values and by monitoring wether or not the loss would be decreasing or increasing we can see if the model is improving or not\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='accuracy', mode='max', verbose=1, min_delta=0.001, patience=5)\n",
    "\n",
    "# Train the model on the training data and evaluate it on the validation data\n",
    "model.fit([input_encoder_train, input_decoder_train], output_decoder_train, epochs=200, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 15s 70ms/step - loss: 5.4765 - accuracy: 0.6028\n",
      "Test loss: [5.476529121398926, 0.6027500033378601]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and store the results\n",
    "\n",
    "loss = model.evaluate([input_encoder_val, input_decoder_val], output_decoder_val)\n",
    "print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten abspeichern"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Model später erneut zu laden wird es in einer Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"models/model_{genre}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Gewichte des Models später im Interferenzmodel zu verwenden, werden diese in einer Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"models/weights_{genre}.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Tokenizer später im Interferenzmodel zu verwenden, wird dieser in einer Datei gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "with open(f\"models/tokenizer_{genre}\", 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verwendung des Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Model zu verwenden muss das run_chatbot.py Script ausgeführt werden. Hier wird mit Hilfe des Interferenzmodells der Chatbot erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\noah\\source\\repos\\ML-Projekt\\run_chatbot.py\", line 17, in <module>\n",
      "    with open(\"models/tokenizer_lesswords.pickle\", \"rb\") as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'models/tokenizer_lesswords.pickle'\n"
     ]
    }
   ],
   "source": [
    "!python run_chatbot.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9978dee63576bf3eb01fd0099124cb1b0d7ef3663923ee832c1b14872d283213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
