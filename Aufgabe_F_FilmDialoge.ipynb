{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe-F-FilmDialoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeines\n",
    "\n",
    "Eine allgemeine Beschreibung der Laboraufgaben inklusive des Vorgehens, den Bewertungsrichtlinien und der Abgabe finden Sie  <a href=\"ML-allgemein.ipynb\">hier</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenquelle\n",
    "\n",
    "\n",
    "* Laden Sie ihre Daten von http://141.72.190.207/ml_lab/F_dialoge herunter\n",
    "    * Die Daten sind geschützt. \n",
    "        * Sie müssen evtl. in einem Netzwerk der DHBW (z.B. WLAN, VPN, ...) angemeldet sein. \n",
    "        * Sie können sich auf der Webseite mit dem Benutzernamen dhbw und dem Zugangsnamen \"ml_LaB_4$\" anmelden. \n",
    "* Die Daten sind in einem anwendungsspezifischen Format gespeichert.\n",
    "    * Sie finden evtl. Informationen über die Daten in einer \"README\" Datei. \n",
    "    * Finden Sie keine solche Datei sind die Daten selbst erklärend. \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten-Sammlung \n",
    "* besteht aus Dialogen aus verschiedensten Filmen\n",
    "* ist in der Readme Datei beschrieben\n",
    "\n",
    "Erstellen Sie aus den einen Chatbot, der auf eine Frage mit einer Antwort im \"Filmjargon\" antwortet! \n",
    "* Verwenden Sie tiefe Neuronale Netze zu Erstellen des Chatbots! \n",
    "* Passen Sie den Chatbot so an, dass er für unterschiedliche Film-Genres unterschiedlich antwortet! \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösung\n",
    "\n",
    "Die Lösung der Aufgabe besteht aus mehreren Teilschritten, welche im Folgenden kurz genannt werden:\n",
    "\n",
    "* Daten einlesen\n",
    "* Daten vorverarbeiten\n",
    "* Model erstellen\n",
    "* Model trainieren\n",
    "* Model abspeichern\n",
    "* Model ausführen\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhängigkeiten des Projekts\n",
    "\n",
    "* Tensorflow 2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "import numpy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Abhängigkeiten des Projekts\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_helper' from 'c:\\\\Users\\\\noah\\\\source\\\\repos\\\\ML-Projekt\\\\src\\\\data_helper.py'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "module_path = str(Path.cwd() / \"src\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import data_helper as dh\n",
    "\n",
    "importlib.reload(dh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_size = 25000\n",
    "max_test_size = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "Hier werden zuerst die rohen Daten eingelesen und innerhalb der Hilfsfunktion \"readDataToLines\" mit des \"newline\" Zeichen getrennt. Die Daten werden zwei Listen gespeichert, welche dann zurückgegeben werden.\n",
    "Verwendet wurden folgende Funktionen:\n",
    "\n",
    "* \"open\" um die Datei zu öffnen\n",
    "* \"read\" um die Datei zu lesen\n",
    "* \"split\" um die Daten anhand des \"newline\" Zeichen zu trennen und in einer Liste zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the files\n",
    "movie_lines, movie_conversations = dh.readDataToLines(\"data/unzipped/movie_lines.txt\", \"data/unzipped/movie_conversations.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_list = dh.readConversationsToList(movie_conversations)\n",
    "\n",
    "# create a dictionary that maps each line id to the corresponding line\n",
    "id2line = dh.readLinesToDict(movie_lines)\n",
    "\n",
    "# remove all unnecessary characters from the lines and replace short forms with the full words\n",
    "id2line = dh.cleanLines(id2line)\n",
    "\n",
    "# split the conversations into requests and responses, each answer is used as a request for the next answer\n",
    "request, response = dh.splitConversationsToRequestAndResponse(conversations_list, id2line)\n",
    "\n",
    "# delete temporary variables\n",
    "del(movie_lines, movie_conversations, conversations_list, id2line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGdCAYAAAASUnlxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz4klEQVR4nO3de1xVdb7/8feWqyBsRYMtR1ImL6OhljgpVkKpqBNZD+aMFh1GHzpq5SWOt9Hx/CaaM4nHeWROesZTZmmp0eNxyk6dJkYsxbzghaS8HXMMbw2oFW68ICiu3x/mGjYXBQT2Zq/X8/FYD9lrffZa3y+rGd6P7/qutWyGYRgCAACwgFbubgAAAEBzIfgAAADLIPgAAADLIPgAAADLIPgAAADLIPgAAADLIPgAAADLIPgAAADL8HV3A5rKtWvX9Pe//10hISGy2Wzubg4AAKgDwzB0/vx5RUZGqlWrxh+f8drg8/e//11RUVHubgYAAGiAkydPqlOnTo2+X68NPiEhIZKu/+JCQ0Pd3BoAAFAXJSUlioqKMv+ONzavDT43Lm+FhoYSfAAAaGGaapoKk5sBAIBlEHwAAIBlEHwAAIBleO0cHwCAdzMMQ1evXlVFRYW7m4J68PHxka+vr9seNUPwAQC0OOXl5SosLNSlS5fc3RQ0QFBQkDp27Ch/f/9mPzbBBwDQoly7dk0FBQXy8fFRZGSk/P39eVBtC2EYhsrLy3X27FkVFBSoW7duTfKQwpsh+AAAWpTy8nJdu3ZNUVFRCgoKcndzUE+tW7eWn5+fjh8/rvLycgUGBjbr8ZncDABokZp7pACNx53njv9qAACAZRB8AACAZTDHBwDgNbrM/bhZj3ds4SPNejx3SU9P1wcffKD8/Hx3N+W2MeIDAICblJeXu7sJlkPwAQCgmSQkJGjq1KmaMWOGOnTooGHDhungwYP6+c9/rjZt2igiIkKpqan67rvvzO9cvHhRv/rVr9SmTRt17NhRL730khISEpSWlmbW2Gw2ffDBBy7Hatu2rVatWmV+/vbbbzVmzBi1a9dO7du312OPPaZjx46Z2zdv3qz77rtPwcHBatu2re6//34dP35cq1at0gsvvKAvv/xSNptNNpvNZb8tDcEHAIBmtHr1avn6+mrbtm1auHCh4uPjdc8992jPnj3KysrS6dOnNXr0aLN+9uzZ2rRpk9avX68NGzZo8+bNysvLq9cxL126pIceekht2rTRli1btHXrVrVp00YjRoxQeXm5rl69qscff1zx8fH66quvtGPHDk2aNEk2m01jxozRzJkzdffdd6uwsFCFhYUaM2ZMY/9amo315vik26V0p7tbAQCwqK5du2rRokWSpN/97nfq16+fFixYYG5/4403FBUVpa+//lqRkZFauXKl3nrrLQ0bNkzS9eDUqVOneh0zMzNTrVq10uuvv24+7PHNN99U27ZttXnzZvXv319Op1NJSUm66667JEk9e/Y0v9+mTRv5+vrK4XDcVt89gfWCDwAAbtS/f3/z57y8PG3atElt2rSpVnf06FGVlpaqvLxccXFx5vqwsDD16NGjXsfMy8vT3/72N4WEhLisv3z5so4eParExESNGzdOw4cP17BhwzR06FCNHj1aHTt2rGfvPB/BBwCAZhQcHGz+fO3aNT366KP6j//4j2p1HTt21JEjR+q0T5vNJsMwXNZduXLF5TixsbFau3Ztte/ecccdkq6PAE2fPl1ZWVl699139W//9m/Kzs7WwIED69SGloLgAwCAm/Tr10/vvfeeunTpIl/f6n+Su3btKj8/P+Xm5urOO++UJBUXF+vrr79WfHy8WXfHHXeosLDQ/HzkyBGXF7j269dP7777rsLDwxUaGlpre+69917de++9mjdvnuLi4rRu3ToNHDhQ/v7+qqioaIwuux2TmwEAcJMpU6bohx9+0JNPPqldu3bpm2++0YYNGzR+/HhVVFSoTZs2mjBhgmbPnq1PP/1U+/fv17hx46q98uHhhx/WsmXL9MUXX2jPnj16+umn5efnZ25/6qmn1KFDBz322GP6/PPPVVBQoJycHD333HM6deqUCgoKNG/ePO3YsUPHjx/Xhg0b9PXXX5vzfLp06aKCggLl5+fru+++U1lZWbP+nhoTwQcAADeJjIzUtm3bVFFRoeHDhysmJkbPPfec7Ha7GW7++Mc/avDgwRo1apSGDh2qBx54QLGxsS77eemllxQVFaXBgwcrJSVFs2bNcnmBa1BQkLZs2aI777xTycnJ6tmzp8aPH6/S0lKFhoYqKChI//d//6df/OIX6t69uyZNmqSpU6dq8uTJkqRf/OIXGjFihB566CHdcccdeuedd5rvl9TIbEbVi4JeoqSkRHa7XU6n03VYj7u6AKBFu3z5sgoKChQdHd3sb/b2FAkJCbrnnnu0ZMkSdzelQW52Dmv9+91IGPEBAACWQfABAACWwV1dAAC0MJs3b3Z3E1osRnwAAIBlEHwAAIBlEHwAAIBlEHwAAIBlEHwAAIBlEHwAAIBl1Cv4pKeny2azuSwOh8PcbhiG0tPTFRkZqdatWyshIUEHDhxw2UdZWZmmTZumDh06KDg4WKNGjdKpU6dcaoqLi5Wamiq73S673a7U1FSdO3eu4b0EAABQA57jc/fdd2vjxo3mZx8fH/PnRYsWafHixVq1apW6d++uP/zhDxo2bJgOHz6skJAQSVJaWpo++ugjZWZmqn379po5c6aSkpKUl5dn7islJUWnTp1SVlaWJGnSpElKTU3VRx99dFudBQB4uXR7Mx+PVyC1NPUOPr6+vi6jPDcYhqElS5Zo/vz5Sk5OliStXr1aERERWrdunSZPniyn06mVK1fq7bff1tChQyVJa9asUVRUlDZu3Kjhw4fr0KFDysrKUm5urgYMGCBJWrFiheLi4nT48GH16NHjdvoLAIDHKC8vl7+/v7ubYSn1nuNz5MgRRUZGKjo6Wk888YS++eYbSVJBQYGKioqUmJho1gYEBCg+Pl7bt2+XJOXl5enKlSsuNZGRkYqJiTFrduzYIbvdboYeSRo4cKDsdrtZU5OysjKVlJS4LAAAeJKEhARNnTpVM2bMUIcOHTRs2DAdPHhQP//5z9WmTRtFREQoNTVV3333nfmd//7v/1bv3r3VunVrtW/fXkOHDtXFixclSePGjdPjjz+uF154QeHh4QoNDdXkyZNVXl5ufr+srEzTp09XeHi4AgMD9cADD2j37t3m9s2bN8tms+nTTz9V//79FRQUpEGDBunw4cNmzZdffqmHHnpIISEhCg0NVWxsrPbs2WNu3759uwYPHqzWrVsrKipK06dPN9voaeoVfAYMGKC33npLf/3rX7VixQoVFRVp0KBB+v7771VUVCRJioiIcPlORESEua2oqEj+/v5q167dTWvCw8OrHTs8PNysqUlGRoY5J8hutysqKqo+XQMAoFmsXr1avr6+2rZtmxYuXKj4+Hjdc8892rNnj7KysnT69GmNHj1aklRYWKgnn3xS48eP16FDh7R582YlJyfLMAxzf59++qkOHTqkTZs26Z133tH69ev1wgsvmNvnzJmj9957T6tXr9YXX3yhrl27avjw4frhhx9c2jV//ny99NJL2rNnj3x9fTV+/Hhz21NPPaVOnTpp9+7dysvL09y5c+Xn5ydJ2rdvn4YPH67k5GR99dVXevfdd7V161ZNnTq1KX+NDVavS10jR440f+7du7fi4uJ01113afXq1Ro4cKAkyWazuXzHMIxq66qqWlNT/a32M2/ePM2YMcP8XFJSQvgBAHicrl27atGiRZKk3/3ud+rXr58WLFhgbn/jjTcUFRWlr7/+WhcuXNDVq1eVnJyszp07S7r+97cyf39/vfHGGwoKCtLdd9+t3//+95o9e7b+/d//XaWlpVq+fLlWrVpl/g1fsWKFsrOztXLlSs2ePdvcz4svvqj4+HhJ0ty5c/XII4/o8uXLCgwM1IkTJzR79mz99Kc/lSR169bN/N4f//hHpaSkKC0tzdz2yiuvKD4+XsuXL1dgYGAj/wZvz23dzh4cHKzevXvryJEj5ryfqqMyZ86cMUeBHA6HysvLVVxcfNOa06dPVzvW2bNnq40mVRYQEKDQ0FCXBQAAT9O/f3/z57y8PG3atElt2rQxlxvh4ujRo+rbt6+GDBmi3r1765e//KVWrFhR7W9o3759FRQUZH6Oi4vThQsXdPLkSR09elRXrlzR/fffb2738/PTfffdp0OHDrnsp0+fPubPHTt2lHT977MkzZgxQ7/+9a81dOhQLVy4UEePHnXpw6pVq1z6MHz4cF27dk0FBQW3++tqdLcVfMrKynTo0CF17NhR0dHRcjgcys7ONreXl5crJydHgwYNkiTFxsbKz8/PpaawsFD79+83a+Li4uR0OrVr1y6zZufOnXI6nWYNAAAtVXBwsPnztWvX9Oijjyo/P99lOXLkiAYPHiwfHx9lZ2frk08+Ua9evbR06VL16NGjToHCZrOZl8TqcjXmxqWryvXXrl2TdP1xNgcOHNAjjzyizz77TL169dL69evNmsmTJ7u0/8svv9SRI0d01113NeA31LTqFXxmzZqlnJwcFRQUaOfOnfrnf/5nlZSUaOzYsbLZbEpLS9OCBQu0fv167d+/X+PGjVNQUJBSUlIkSXa7XRMmTNDMmTP16aefau/evfqXf/kX9e7d27zLq2fPnhoxYoQmTpyo3Nxc5ebmauLEiUpKSuKOLgCAV+nXr58OHDigLl26qGvXri7LjYBks9l0//3364UXXtDevXvl7+9vhg7p+sTj0tJS83Nubq7atGmjTp06qWvXrvL399fWrVvN7VeuXNGePXvUs2fPerW1e/fu+td//Vdt2LBBycnJevPNN136ULX9N47taeoVfE6dOqUnn3xSPXr0UHJysvz9/ZWbm2ted5wzZ47S0tL07LPPqn///vr222+1YcMG8xk+kvTyyy/r8ccf1+jRo3X//fcrKChIH330kcvzgNauXavevXsrMTFRiYmJ6tOnj95+++1G6jIAAJ5hypQp+uGHH/Tkk09q165d+uabb7RhwwaNHz9eFRUV2rlzpxYsWKA9e/boxIkTev/993X27FmX0FJeXq4JEybo4MGD+uSTT/T8889r6tSpatWqlYKDg/XMM89o9uzZysrK0sGDBzVx4kRdunRJEyZMqFMbS0tLNXXqVG3evFnHjx/Xtm3btHv3brMNv/nNb7Rjxw5NmTLFHK368MMPNW3atCb5nd2uek1uzszMvOl2m82m9PR0paen11oTGBiopUuXaunSpbXWhIWFac2aNfVpGgAALU5kZKS2bdum3/zmNxo+fLjKysrUuXNnjRgxQq1atVJoaKi2bNmiJUuWqKSkRJ07d9ZLL73kcrPRkCFD1K1bNw0ePFhlZWV64oknXP4OL1y4UNeuXVNqaqrOnz+v/v37669//Wu1O6xr4+Pjo++//16/+tWvdPr0aXXo0EHJycnmnWN9+vRRTk6O5s+frwcffFCGYeiuu+7SmDFjGvV31VhsRuV74rxISUmJ7Ha7nE6n60TndDtP2gSAFuzy5csqKChQdHS0x90x1NzGjRunc+fO6YMPPnB3U+rlZuew1r/fjYSXlAIAAMsg+AAAAMuo97u6AACAZ1i1apW7m9DiMOIDAAAsg+ADAAAsg+ADAGiRvPSmZEtw57kj+AAAWpQbr1a4dOmSm1uChrpx7iq/JqO5MLkZANCi+Pj4qG3btuYLNIOCgqq9dwqeyTAMXbp0SWfOnFHbtm1d3trQXAg+AIAWx+FwSPrH28PRsrRt29Y8h82N4AMAaHFsNps6duyo8PBwXblyxd3NQT34+fm5ZaTnBoIPAKDF8vHxcesfUbQ8TG4GAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfABAACWQfCphy5zP3b5FwAAtCwEHwAAYBkEHwAAYBkEHwAAYBkEHwAAYBkEHwAAYBkEHwAAYBkEn3rilnYAAFougg8AALAMgg8AALAMgg8AALCM2wo+GRkZstlsSktLM9cZhqH09HRFRkaqdevWSkhI0IEDB1y+V1ZWpmnTpqlDhw4KDg7WqFGjdOrUKZea4uJipaamym63y263KzU1VefOnbud5gIAAItrcPDZvXu3XnvtNfXp08dl/aJFi7R48WItW7ZMu3fvlsPh0LBhw3T+/HmzJi0tTevXr1dmZqa2bt2qCxcuKCkpSRUVFWZNSkqK8vPzlZWVpaysLOXn5ys1NbWhzb21dHvT7RsAAHiEBgWfCxcu6KmnntKKFSvUrl07c71hGFqyZInmz5+v5ORkxcTEaPXq1bp06ZLWrVsnSXI6nVq5cqVeeuklDR06VPfee6/WrFmjffv2aePGjZKkQ4cOKSsrS6+//rri4uIUFxenFStW6H//9391+PDhRug2AACwogYFnylTpuiRRx7R0KFDXdYXFBSoqKhIiYmJ5rqAgADFx8dr+/btkqS8vDxduXLFpSYyMlIxMTFmzY4dO2S32zVgwACzZuDAgbLb7WZNVWVlZSopKXFZbhe3rAMA4F3qHXwyMzP1xRdfKCMjo9q2oqIiSVJERITL+oiICHNbUVGR/P39XUaKaqoJDw+vtv/w8HCzpqqMjAxzPpDdbldUVFR9u3ZLxwJTGn2fAACg+dQr+Jw8eVLPPfec1qxZo8DAwFrrbDaby2fDMKqtq6pqTU31N9vPvHnz5HQ6zeXkyZM3PR4AALCeegWfvLw8nTlzRrGxsfL19ZWvr69ycnL0yiuvyNfX1xzpqToqc+bMGXObw+FQeXm5iouLb1pz+vTpasc/e/ZstdGkGwICAhQaGuqyAAAAVFav4DNkyBDt27dP+fn55tK/f3899dRTys/P109+8hM5HA5lZ2eb3ykvL1dOTo4GDRokSYqNjZWfn59LTWFhofbv32/WxMXFyel0ateuXWbNzp075XQ6zZrmxFwfAAC8g299ikNCQhQTE+OyLjg4WO3btzfXp6WlacGCBerWrZu6deumBQsWKCgoSCkp1+fH2O12TZgwQTNnzlT79u0VFhamWbNmqXfv3uZk6Z49e2rEiBGaOHGiXn31VUnSpEmTlJSUpB49etx2p+vq+pweZ7MdDwAANK16BZ+6mDNnjkpLS/Xss8+quLhYAwYM0IYNGxQSEmLWvPzyy/L19dXo0aNVWlqqIUOGaNWqVfLx8TFr1q5dq+nTp5t3f40aNUrLli1r7OYCAAALsRmGYbi7EU2hpKREdrtdTqfTdb5Pul1Kr2EUp6b1P67rMvdjHVv4iPmQwy6Xrz+T6NjCR5qq+QAAWFKtf78bCe/qagTMAQIAoGUg+AAAAMsg+NQBDy4EAMA7EHwAAIBlEHwaEXN9AADwbI1+O7sV/OPSl9MMO4QeAAA8HyM+AADAMgg+jYyRHwAAPJd1gs+PDx8EAADWZZ3gUxPCEAAAlmLt4AMAACyF4HMLzNkBAMB7EHxqQeABAMD7EHwAAIBlEHyqasCE55re5dVl7seMGgEA4GEIPgAAwDIIPgAAwDIIPgAAwDIIPrfpxvyemub5AAAAz0LwuQUCDQAA3oPgAwAALIPgAwAALIPgAwAALIPgU1mlhxc2dG5P5e8xPwgAAM9C8AEAAJZB8AEAAJZB8AEAAJZB8GlEzOkBAMCzEXwAAIBlEHyaWJe5H7u7CQAA4EcEnybCZS8AADwPwQcAAFgGwQcAAFgGwQcAAFgGwQcAAFgGwacZcGcXAACegeADAAAsg+ADAAAsg+ADAAAsg+DTjJjrAwCAexF8AACAZRB8mhivrgAAwHMQfAAAgGUQfAAAgGUQfJoZE5wBAHAfgg8AALAMgg8AALAMgg8AALAMgk8z4tZ2AADci+DTzAg/AAC4D8EHAABYBsEHAABYBsFHktLtTbp7Lm8BAOAZCD4AAMAyCD4AAMAy6hV8li9frj59+ig0NFShoaGKi4vTJ598Ym43DEPp6emKjIxU69atlZCQoAMHDrjso6ysTNOmTVOHDh0UHBysUaNG6dSpUy41xcXFSk1Nld1ul91uV2pqqs6dO9fwXnoYXlsBAIB71Cv4dOrUSQsXLtSePXu0Z88ePfzww3rsscfMcLNo0SItXrxYy5Yt0+7du+VwODRs2DCdP3/e3EdaWprWr1+vzMxMbd26VRcuXFBSUpIqKirMmpSUFOXn5ysrK0tZWVnKz89XampqI3UZAABYlW99ih999FGXzy+++KKWL1+u3Nxc9erVS0uWLNH8+fOVnJwsSVq9erUiIiK0bt06TZ48WU6nUytXrtTbb7+toUOHSpLWrFmjqKgobdy4UcOHD9ehQ4eUlZWl3NxcDRgwQJK0YsUKxcXF6fDhw+rRo0dj9BsAAFhQg+f4VFRUKDMzUxcvXlRcXJwKCgpUVFSkxMREsyYgIEDx8fHavn27JCkvL09XrlxxqYmMjFRMTIxZs2PHDtntdjP0SNLAgQNlt9vNmpqUlZWppKTEZfFkXO4CAKD51Tv47Nu3T23atFFAQICefvpprV+/Xr169VJRUZEkKSIiwqU+IiLC3FZUVCR/f3+1a9fupjXh4eHVjhseHm7W1CQjI8OcE2S32xUVFVXfrgEAAC9X7+DTo0cP5efnKzc3V88884zGjh2rgwcPmtttNptLvWEY1dZVVbWmpvpb7WfevHlyOp3mcvLkybp2qdnxXB8AANyj3sHH399fXbt2Vf/+/ZWRkaG+ffvqT3/6kxwOhyRVG5U5c+aMOQrkcDhUXl6u4uLim9acPn262nHPnj1bbTSpsoCAAPNusxsLAABAZbf9HB/DMFRWVqbo6Gg5HA5lZ2eb28rLy5WTk6NBgwZJkmJjY+Xn5+dSU1hYqP3795s1cXFxcjqd2rVrl1mzc+dOOZ1OswYAAKAh6nVX129/+1uNHDlSUVFROn/+vDIzM7V582ZlZWXJZrMpLS1NCxYsULdu3dStWzctWLBAQUFBSkm5fmnHbrdrwoQJmjlzptq3b6+wsDDNmjVLvXv3Nu/y6tmzp0aMGKGJEyfq1VdflSRNmjRJSUlJ3NEFAABuS72Cz+nTp5WamqrCwkLZ7Xb16dNHWVlZGjZsmCRpzpw5Ki0t1bPPPqvi4mINGDBAGzZsUEhIiLmPl19+Wb6+vho9erRKS0s1ZMgQrVq1Sj4+PmbN2rVrNX36dPPur1GjRmnZsmWN0V8AAGBh9Qo+K1euvOl2m82m9PR0paen11oTGBiopUuXaunSpbXWhIWFac2aNfVpGgAAwC3xri4AAGAZ1g0+6Xa3Hp5b2gEAaH7WDT4AAMByCD4AAMAyCD4AAMAyCD4AAMAyCD5uxlvaAQBoPgQfAABgGQQfAABgGZYMPlxeAgDAmiwZfAAAgDURfAAAgGVYMvh40usiPKktAAB4O0sGHwAAYE0EHwAAYBkEHwAAYBkEHwAAYBkEHw/Bs4UAAGh6BB8AAGAZBB8AAGAZBB8AAGAZBB9PkG53dwsAALAEgg8AALAMgg8AALAMgg8AALAMgg8AALAMgg8AALAMgg8AALAMgg8AALAMgo+HOBaYwvu6AABoYgQfAABgGQQfAABgGQQfAABgGQQfD8RcHwAAmgbBBwAAWAbBBwAAWAbBx4McC0xxdxMAAPBq1gg+6XZ3t6DemOcDAEDjs0bwAQAAEMEHAABYCMHHAzHXBwCApkHwAQAAlkHwAQAAlkHwAQAAlkHwAQAAlkHwAQAAlkHw8VDc2QUAQOMj+AAAAMsg+HgwXlsBAEDjIvgAAADLIPh4MOb5AADQuAg+AADAMgg+Ho55PgAANB6Cj4e7cbmLAAQAwO0j+LQghB8AAG5PvYJPRkaGfvaznykkJETh4eF6/PHHdfjwYZcawzCUnp6uyMhItW7dWgkJCTpw4IBLTVlZmaZNm6YOHTooODhYo0aN0qlTp1xqiouLlZqaKrvdLrvdrtTUVJ07d65hvQQAAFA9g09OTo6mTJmi3NxcZWdn6+rVq0pMTNTFixfNmkWLFmnx4sVatmyZdu/eLYfDoWHDhun8+fNmTVpamtavX6/MzExt3bpVFy5cUFJSkioqKsyalJQU5efnKysrS1lZWcrPz1dqamojdBkAAFiVb32Ks7KyXD6/+eabCg8PV15engYPHizDMLRkyRLNnz9fycnJkqTVq1crIiJC69at0+TJk+V0OrVy5Uq9/fbbGjp0qCRpzZo1ioqK0saNGzV8+HAdOnRIWVlZys3N1YABAyRJK1asUFxcnA4fPqwePXo0Rt9blGOBKepyeZ27mwEAQIt2W3N8nE6nJCksLEySVFBQoKKiIiUmJpo1AQEBio+P1/bt2yVJeXl5unLliktNZGSkYmJizJodO3bIbreboUeSBg4cKLvdbtZUVVZWppKSEpcFAACgsgYHH8MwNGPGDD3wwAOKiYmRJBUVFUmSIiIiXGojIiLMbUVFRfL391e7du1uWhMeHl7tmOHh4WZNVRkZGeZ8ILvdrqioqIZ2zWPxQEMAAG5Pg4PP1KlT9dVXX+mdd96pts1ms7l8Ngyj2rqqqtbUVH+z/cybN09Op9NcTp48WZduAAAAC2lQ8Jk2bZo+/PBDbdq0SZ06dTLXOxwOSao2KnPmzBlzFMjhcKi8vFzFxcU3rTl9+nS14549e7baaNINAQEBCg0NdVkAAAAqq1fwMQxDU6dO1fvvv6/PPvtM0dHRLtujo6PlcDiUnZ1trisvL1dOTo4GDRokSYqNjZWfn59LTWFhofbv32/WxMXFyel0ateuXWbNzp075XQ6zRoAAID6qtddXVOmTNG6dev0P//zPwoJCTFHdux2u1q3bi2bzaa0tDQtWLBA3bp1U7du3bRgwQIFBQUpJSXFrJ0wYYJmzpyp9u3bKywsTLNmzVLv3r3Nu7x69uypESNGaOLEiXr11VclSZMmTVJSUpIl7+gCAACNo17BZ/ny5ZKkhIQEl/Vvvvmmxo0bJ0maM2eOSktL9eyzz6q4uFgDBgzQhg0bFBISYta//PLL8vX11ejRo1VaWqohQ4Zo1apV8vHxMWvWrl2r6dOnm3d/jRo1SsuWLWtIHwEAACTVM/gYhnHLGpvNpvT0dKWnp9daExgYqKVLl2rp0qW11oSFhWnNmjX1aZ4ldJn7sY4tfMTdzQAAoEXiXV0tDLe0AwDQcAQfAABgGQQfAABgGQQfAABgGQSfFqjL3I/d3QQAAFokgg8AALAMgk9Llm53dwsAAGhRCD4AAMAyCD4AAMAyCD4tUOWHGDLRGQCAuiP4AAAAyyD4AAAAyyD4tHC8uwsAgLoj+AAAAMsg+AAAAMsg+HgLHmYIAMAtEXy8ALe0AwBQNwQfAABgGQQfL8UoEAAA1RF8vAC3tAMAUDcEHy9FGAIAoDqCjzf58c4uLnMBAFAzgo8XYrQHAICaEXwAAIBlEHwAAIBlEHy8GHN9AABwRfABAACWQfABAACWQfDxYi53d/ESUwAACD7erqZ5Psz9AQBYFcHHgnjODwDAqgg+VsMlLwCAhRF8AACAZRB8AACAZRB8vNwt5/Nw6QsAYCEEHwvhbi4AgNURfCyk2ugPoz0AAIsh+AAAAMsg+AAAAMsg+AAAAMsg+AAAAMsg+AAAAMsg+IC7uwAAlkHwwXWEHwCABRB8AACAZRB8AACAZRB8AACAZRB84IL3eQEAvBnBBy6OBab8Y6IzE54BAF6G4AMAACyD4AMAACyD4IOacZkLAOCFCD4AAMAyCD4AAMAyCD4AAMAyCD4AAMAy6h18tmzZokcffVSRkZGy2Wz64IMPXLYbhqH09HRFRkaqdevWSkhI0IEDB1xqysrKNG3aNHXo0EHBwcEaNWqUTp065VJTXFys1NRU2e122e12paam6ty5c/XuIAAAwA31Dj4XL15U3759tWzZshq3L1q0SIsXL9ayZcu0e/duORwODRs2TOfPnzdr0tLStH79emVmZmrr1q26cOGCkpKSVFFRYdakpKQoPz9fWVlZysrKUn5+vlJTUxvQRQAAgOt86/uFkSNHauTIkTVuMwxDS5Ys0fz585WcnCxJWr16tSIiIrRu3TpNnjxZTqdTK1eu1Ntvv62hQ4dKktasWaOoqCht3LhRw4cP16FDh5SVlaXc3FwNGDBAkrRixQrFxcXp8OHD6tGjR0P7iwboMvdjHQtMUZfL6358srPT3U0CAKBBGnWOT0FBgYqKipSYmGiuCwgIUHx8vLZv3y5JysvL05UrV1xqIiMjFRMTY9bs2LFDdrvdDD2SNHDgQNntdrOmqrKyMpWUlLgsaBzHAlNc/pV4pxcAoGVq1OBTVFQkSYqIiHBZHxERYW4rKiqSv7+/2rVrd9Oa8PDwavsPDw83a6rKyMgw5wPZ7XZFRUXddn8AAIB3aZK7umw2m8tnwzCqrauqak1N9Tfbz7x58+R0Os3l5MmTDWg56qry6A8AAC1FowYfh8MhSdVGZc6cOWOOAjkcDpWXl6u4uPimNadPn662/7Nnz1YbTbohICBAoaGhLgsAAEBljRp8oqOj5XA4lJ2dba4rLy9XTk6OBg0aJEmKjY2Vn5+fS01hYaH2799v1sTFxcnpdGrXrl1mzc6dO+V0Os0aeADe5wUAaGHqfVfXhQsX9Le//c38XFBQoPz8fIWFhenOO+9UWlqaFixYoG7duqlbt25asGCBgoKClJJy/dKI3W7XhAkTNHPmTLVv315hYWGaNWuWevfubd7l1bNnT40YMUITJ07Uq6++KkmaNGmSkpKSuKMLAAA0WL2Dz549e/TQQw+Zn2fMmCFJGjt2rFatWqU5c+aotLRUzz77rIqLizVgwABt2LBBISEh5ndefvll+fr6avTo0SotLdWQIUO0atUq+fj4mDVr167V9OnTzbu/Ro0aVeuzg+A+N2515xZ3AEBLUO/gk5CQIMMwat1us9mUnp6u9PT0WmsCAwO1dOlSLV26tNaasLAwrVmzpr7NQzNjkjMAoCXhXV0AAMAyCD5oXEx4BgB4MIIPmg4hCADgYQg+aBQur7Ag8AAAPBTBB42i2iRnwg8AwAMRfAAAgGUQfNAseJs7AMATEHzQ5MyHHFbF5TAAQDMj+AAAAMsg+KDJ8XRnAICnIPigeXF5CwDgRgQfuAcBCADgBgQfNB/CDgDAzQg+cL8bgYhgBABoYgQfAABgGQQfuFfVUR5GfQAATYjgA89D+AEANBGCDzwfQQgA0EgIPvBMTHgGADQBgg9aBgIQAKAREHzQchB+AAC3ieCDlokQBABoAF93NwCotyrzf7pcXqdjCx9xY4MAAC0FIz4AAMAyCD5o8Y4Fpri7CQCAFoLgA++RbmfuDwDgpgg+8H6EIQDAjwg+8D6M/AAAakHwgXfjCdAAgEoIPgAAwDIIPrAmRoAAwJIIPrCOqmGn8lwgghAAWALBB9ZDyAEAyyL4ADcQiADA6xF8gMoqhx+CEAB4HYIPUBXhBwC8FsEHuBXCDwB4DYIPUA9d5n5c851ghCMAaBEIPkBd/BhszDfB1xR0uDUeADwewQdoDoQhAPAIBB/gdlQJNF3mfuymhgAA6oLgAzQi81JYZcwFAgCPQfABmojLROjKKr8qo/K6yv8CAJoEwQdoIjWO/lRG2AGAZkfwATxFTZfEuEwGAI2K4AN4qtpGhG4WgAhHAHBTBB+gJbnZCBChBwBuydfdDQDQAFUCUJfL63QssHpZl7kfX59rlO6s8h1n07cRADwQIz6AF6g2kfrHO8dqm2Dt8rwhnkUEwEIIPoAFVXv1RqXwU+NrObiMBsBLEHwAVFffidUEIwAtBMEHwHW3CC/mJbCqD2CsT+ip6eGNANCMCD4A6qTOr+O4EW4qhZxan2Jdl/0BQCMi+ABocjWFJjMM1TSCVFsA4lUfAG4Tt7MDcItbvtKjKpcRJWfNoadaMHLWUO+sua7yvgF4LY8f8fnzn/+s6OhoBQYGKjY2Vp9//rm7mwSgud3uiE59LrNVHnGqaUTqVuvre9yG1AJoMI8OPu+++67S0tI0f/587d27Vw8++KBGjhypEydO1H0nGZ2aroEALKPWeUpVR55qu3RXU11N+6jLz1W/f7M2A3Dh0cFn8eLFmjBhgn7961+rZ8+eWrJkiaKiorR8+XJ3Nw2AxdT70tzN3G54qS1sVQlI15/aXcsI1i3W1WsO1s0ec1DXuwXroq6jYrzPDjfhsXN8ysvLlZeXp7lz57qsT0xM1Pbt26vVl5WVqayszPzsdF6/Tl9SZvyjqKREqvy5KdY113E4Nsfm2JY49le2J1VS1vDvN3Sdy3FvrK88gj4vVJp36h/fnxf647+nqo+0V/1upbqvbFLJPNX+3arrb3acutbdpD03PXZt6+aduv5vRifFXF6p/YETXOpiLq/U/heGm+Uxz//1es2N7/343WrHqLz+xjEr/3tje+X91LAu5vm//uP4NdXf6jg1bau8rxr6eDtKSkokSYZRw//+GoPhob799ltDkrFt2zaX9S+++KLRvXv3avXPP/+8IYmFhYWFhYXFC5ajR482Sb7w2BGfG2w2m8tnwzCqrZOkefPmacaMGebnc+fOqXPnzjpx4oTsdnuTt9NTlJSUKCoqSidPnlRoaKi7m9Ns6Df9tgL6Tb+twOl06s4771RYWFiT7N9jg0+HDh3k4+OjoqIil/VnzpxRREREtfqAgAAFBARUW2+32y31H8wNoaGh9NtC6Le10G9rsWq/W7VqmmnIHju52d/fX7GxscrOznZZn52drUGDBrmpVQAAoCXz2BEfSZoxY4ZSU1PVv39/xcXF6bXXXtOJEyf09NNPu7tpAACgBfLo4DNmzBh9//33+v3vf6/CwkLFxMToL3/5izp37nzL7wYEBOj555+v8fKXN6Pf9NsK6Df9tgL63TT9thlGU90vBgAA4Fk8do4PAABAYyP4AAAAyyD4AAAAyyD4AAAAy/Da4PPnP/9Z0dHRCgwMVGxsrD7//HN3N6nRpKeny2azuSwOh8PcbhiG0tPTFRkZqdatWyshIUEHDhxwY4sbZsuWLXr00UcVGRkpm82mDz74wGV7XfpZVlamadOmqUOHDgoODtaoUaN06lQN76nxILfq97hx46qd/4EDB7rUtMR+Z2Rk6Gc/+5lCQkIUHh6uxx9/XIcPH3ap8cZzXpd+e+M5X758ufr06WM+nC8uLk6ffPKJud0bz7V0635747muSUZGhmw2m9LS0sx1zXXOvTL4vPvuu0pLS9P8+fO1d+9ePfjggxo5cqROnDjh7qY1mrvvvluFhYXmsm/fPnPbokWLtHjxYi1btky7d++Ww+HQsGHDdP78eTe2uP4uXryovn37atmyZTVur0s/09LStH79emVmZmrr1q26cOGCkpKSVFFR0VzdqLdb9VuSRowY4XL+//KXv7hsb4n9zsnJ0ZQpU5Sbm6vs7GxdvXpViYmJunjxolnjjee8Lv2WvO+cd+rUSQsXLtSePXu0Z88ePfzww3rsscfMP3TeeK6lW/db8r5zXdXu3bv12muvqU+fPi7rm+2cN8kbwNzsvvvuM55++mmXdT/96U+NuXPnuqlFjev55583+vbtW+O2a9euGQ6Hw1i4cKG57vLly4bdbjf+67/+q5la2PgkGevXrzc/16Wf586dM/z8/IzMzEyz5ttvvzVatWplZGVlNVvbb0fVfhuGYYwdO9Z47LHHav2ON/TbMAzjzJkzhiQjJyfHMAzrnPOq/TYM65zzdu3aGa+//rplzvUNN/ptGN5/rs+fP29069bNyM7ONuLj443nnnvOMIzm/d+31434lJeXKy8vT4mJiS7rExMTtX37dje1qvEdOXJEkZGRio6O1hNPPKFvvvlGklRQUKCioiKX/gcEBCg+Pt6r+l+Xfubl5enKlSsuNZGRkYqJiWnxv4vNmzcrPDxc3bt318SJE3XmzBlzm7f02+l0SpL5okKrnPOq/b7Bm895RUWFMjMzdfHiRcXFxVnmXFft9w3efK6nTJmiRx55REOHDnVZ35zn3KOf3NwQ3333nSoqKqq9yDQiIqLaC09bqgEDBuitt95S9+7ddfr0af3hD3/QoEGDdODAAbOPNfX/+PHj7mhuk6hLP4uKiuTv76927dpVq2nJ/y2MHDlSv/zlL9W5c2cVFBTo//2//6eHH35YeXl5CggI8Ip+G4ahGTNm6IEHHlBMTIwka5zzmvotee8537dvn+Li4nT58mW1adNG69evV69evcw/Yt56rmvrt+S951qSMjMz9cUXX2j37t3VtjXn/769LvjcYLPZXD4bhlFtXUs1cuRI8+fevXsrLi5Od911l1avXm1OgvPm/lfWkH629N/FmDFjzJ9jYmLUv39/de7cWR9//LGSk5Nr/V5L6vfUqVP11VdfaevWrdW2efM5r63f3nrOe/Toofz8fJ07d07vvfeexo4dq5ycHHO7t57r2vrdq1cvrz3XJ0+e1HPPPacNGzYoMDCw1rrmOOded6mrQ4cO8vHxqZb+zpw5Uy1Jeovg4GD17t1bR44cMe/u8vb+16WfDodD5eXlKi4urrXGG3Ts2FGdO3fWkSNHJLX8fk+bNk0ffvihNm3apE6dOpnrvf2c19bvmnjLOff391fXrl3Vv39/ZWRkqG/fvvrTn/7k9ee6tn7XxFvOdV5ens6cOaPY2Fj5+vrK19dXOTk5euWVV+Tr62u2vTnOudcFH39/f8XGxio7O9tlfXZ2tgYNGuSmVjWtsrIyHTp0SB07dlR0dLQcDodL/8vLy5WTk+NV/a9LP2NjY+Xn5+dSU1hYqP3793vV7+L777/XyZMn1bFjR0ktt9+GYWjq1Kl6//339dlnnyk6Otplu7ee81v1uybecs6rMgxDZWVlXnuua3Oj3zXxlnM9ZMgQ7du3T/n5+ebSv39/PfXUU8rPz9dPfvKT5jvnDZiU7fEyMzMNPz8/Y+XKlcbBgweNtLQ0Izg42Dh27Ji7m9YoZs6caWzevNn45ptvjNzcXCMpKckICQkx+7dw4ULDbrcb77//vrFv3z7jySefNDp27GiUlJS4ueX1c/78eWPv3r3G3r17DUnG4sWLjb179xrHjx83DKNu/Xz66aeNTp06GRs3bjS++OIL4+GHHzb69u1rXL161V3duqWb9fv8+fPGzJkzje3btxsFBQXGpk2bjLi4OOOf/umfWny/n3nmGcNutxubN282CgsLzeXSpUtmjTee81v121vP+bx584wtW7YYBQUFxldffWX89re/NVq1amVs2LDBMAzvPNeGcfN+e+u5rk3lu7oMo/nOuVcGH8MwjP/8z/80OnfubPj7+xv9+vVzuTW0pRszZozRsWNHw8/Pz4iMjDSSk5ONAwcOmNuvXbtmPP/884bD4TACAgKMwYMHG/v27XNjixtm06ZNhqRqy9ixYw3DqFs/S0tLjalTpxphYWFG69atjaSkJOPEiRNu6E3d3azfly5dMhITE4077rjD8PPzM+68805j7Nix1frUEvtdU58lGW+++aZZ443n/Fb99tZzPn78ePP/o++44w5jyJAhZugxDO8814Zx835767muTdXg01zn3GYYhlHvMSsAAIAWyOvm+AAAANSG4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACyD4AMAACzj/wNGcP1aGS01WQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the data into a list of conversations\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# get the length of each line of the requests and save the occurrences in a dictionary\n",
    "lengths_request = {}\n",
    "for sentence in request:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_request:\n",
    "        lengths_request[length] += 1\n",
    "    else:\n",
    "        lengths_request[length] = 1\n",
    "\n",
    "# get the length of each line of the responses and save the occurrences in a dictionary\n",
    "lengths_response = {}\n",
    "for sentence in response:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_response:\n",
    "        lengths_response[length] += 1\n",
    "    else:\n",
    "        lengths_response[length] = 1\n",
    "\n",
    "# plot the occurrences of the lengths of the requests and responses in the same plot while only showind x values up to 200\n",
    "plt.bar(lengths_request.keys(), lengths_request.values(), label=\"request\")\n",
    "plt.bar(lengths_response.keys(), lengths_response.values(), label=\"response\")\n",
    "plt.xlim(0, 400)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# based on the result set the parameters\n",
    "max_wordcount_in_sentence = 200\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man in der obigen Grafik erkennt, sind ab einer Sequenzlänge von 200 nur noch wenige Daten vorhanden. Da die Sequenzlänge maßgeblich auch für den Speicherbedarf in weiteren Schritten ist, wird diese hier auf 200 begrenzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove every request and its corresponding response if one of them has more words than max_wordcount_in_sentence\n",
    "request, response = dh.removeLongSequences(request, response, max_wordcount_in_sentence)\n",
    "\n",
    "# limit the size of the dataset\n",
    "test_request = request[max_train_size:max_train_size+max_test_size]\n",
    "test_response = response[max_train_size:max_train_size+max_test_size]\n",
    "\n",
    "request = request[0:max_train_size]\n",
    "response = response[0:max_train_size]\n",
    "\n",
    "\n",
    "# get a dictionary of all unique words with their frequency\n",
    "word2count = dh.getWord2Count(request, response)\n",
    "\n",
    "# filter out words with a frequency of 5 or less\n",
    "min_wordFrequency = 5\n",
    "word2count = {k:v for k,v in word2count.items() if v > min_wordFrequency}\n",
    "\n",
    "del(min_wordFrequency)\n",
    "\n",
    "# create a dictionary that maps each word to a unique integer, used in the encoder\n",
    "# tokens: <S> (Start), <E> (End), <N> (Null), <F> (Filler)\n",
    "# add the tokens to the front of the dictionary to optimize searching while shifting the indices of the other words by 4\n",
    "word2index = {}\n",
    "word2index[\"<F>\"] = 0\n",
    "word2index[\"<S>\"] = 1\n",
    "word2index[\"<E>\"] = 2\n",
    "word2index[\"<N>\"] = 3\n",
    "word2index |= {k:v+4 for v,k in enumerate(word2count.keys())} #merge dictionaries\n",
    "# this line of code creates a dictionary word2index by using a dictionary comprehension\n",
    "# the comprehension iterates over the keys of the dictionary word2count and assigns each key a value that is the index of the key plus 4 (the first 4 indices are reserved for the tokens)\n",
    "# v takes the values 0, 1, 2, ... and k takes the values of the keys of the word2count dictionary\n",
    "# the enumerate function generates a sequence of index, value pairs from the keys of the word2count dictionary\n",
    "# the dictionary comprehension creates a new dictionary with the keys from word2count as the keys of word2index and the values of word2index being the index of the keys in word2count plus 4.\n",
    "\n",
    "# invert the dictionary word2index to create a dictionary that maps each integer to a unique word, used in the decoder\n",
    "index2word = {k:v for v,k in word2index.items()}\n",
    "\n",
    "# save the dictionaries to the disk to use them later\n",
    "dh.saveDictionary(word2index, \"data/word2index.dict\")\n",
    "dh.saveDictionary(index2word, \"data/index2word.dict\")\n",
    "\n",
    "# encapsule the responses with the tokens <S> (Start) and <E> (End)\n",
    "response = dh.encapsuleWithTokens(response, \"<S>\", \"<E>\")\n",
    "\n",
    "\n",
    "\n",
    "del(word2count)\n",
    "\n",
    "# translate the inputs to numeric values and uses the null token <N> when the word is not in the dictionary\n",
    "inputEncoder = dh.translateToNumeric(request, word2index, \"<N>\") \n",
    "inputDecoder = dh.translateToNumeric(response, word2index, \"<N>\") \n",
    "test_inputEncoder = dh.translateToNumeric(test_request, word2index, \"<N>\")\n",
    "test_inputDecoder = dh.translateToNumeric(test_response, word2index, \"<N>\")\n",
    "\n",
    "# remove start token from decoder input so that the decoder doesn't learn to just copy the start token to each output, don't remove the end token because the decoder should learn when to stop with generating output\n",
    "outputDecoder = dh.removeStartToken(inputDecoder)\n",
    "test_outputDecoder = dh.removeStartToken(test_inputDecoder)\n",
    "\n",
    "del(request, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the maximum length of the requests\n",
    "max_length_request = max([len(x) for x in inputEncoder])\n",
    "# get the maximum length of the responses\n",
    "max_length_response = max([len(x) for x in inputDecoder])\n",
    "# get the maximum length of the test requests\n",
    "max_length_test_request = max([len(x) for x in test_inputEncoder])\n",
    "# get the maximum length of the test responses\n",
    "max_length_test_response = max([len(x) for x in test_inputDecoder])\n",
    "# get the maximum length of the requests, responses and test requests and responses\n",
    "max_length = max(max_length_request, max_length_response, max_length_test_request, max_length_test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the keras function pad_sequences to pad the data with zeros (which represent a filling token) to the maximum length\n",
    "inputEncoder = tf.keras.preprocessing.sequence.pad_sequences(inputEncoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "inputDecoder = tf.keras.preprocessing.sequence.pad_sequences(inputDecoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "outputDecoder = tf.keras.preprocessing.sequence.pad_sequences(outputDecoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "test_inputEncoder = tf.keras.preprocessing.sequence.pad_sequences(test_inputEncoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "test_inputDecoder = tf.keras.preprocessing.sequence.pad_sequences(test_inputDecoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "test_outputDecoder = tf.keras.preprocessing.sequence.pad_sequences(test_outputDecoder, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# convert the decoder input to a one-hot encoded vector which the model can use\n",
    "outputDecoder = tf.keras.utils.to_categorical(outputDecoder, num_classes=len(word2index), dtype=\"float32\") # using float32 even though it's memory intensive because later steps need it to be float32\n",
    "test_outputDecoder = tf.keras.utils.to_categorical(test_outputDecoder, num_classes=len(word2index), dtype=\"float32\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model erstellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Erzeugen der einzelnen Layer ist in der Reihenfolge implementiert, nach welcher Daten das Model durchlaufen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.2838WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 609s 773ms/step - loss: 1.2838\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 1.0478WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 599s 767ms/step - loss: 1.0478\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.9849WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 618s 790ms/step - loss: 0.9849\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.9514WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 599s 766ms/step - loss: 0.9514\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.9290WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 593s 759ms/step - loss: 0.9290\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.9108WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 583s 746ms/step - loss: 0.9108\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8952WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 595s 760ms/step - loss: 0.8952\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8810WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 582s 745ms/step - loss: 0.8810\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.8678WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "782/782 [==============================] - 608s 778ms/step - loss: 0.8678\n",
      "Epoch 10/10\n",
      "500/782 [==================>...........] - ETA: 4:02 - loss: 0.8520"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "outputDimension = 50 \n",
    "lstm_units = 256\n",
    "\n",
    "#encoder input, embedding and lstm\n",
    "\n",
    "# input tensor for the encoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputEncoderTensor = tf.keras.Input(shape=(max_length, ), name = \"inputEncoderTensor\")\n",
    "\n",
    "# embedding layer of the encoder, the input is the input tensor, the output is the embedding tensor\n",
    "encoderEmbedding = tf.keras.layers.Embedding(len(word2index) + 1, output_dim = outputDimension, input_length = max_length, trainable = True, name = \"embeddingEncoderLayer\")(inputEncoderTensor)\n",
    "\n",
    "# LSTM layer of the encoder, the input is the embedding tensor, the output is the output tensor and the hidden state of the encoder\n",
    "encoderLSTM, encoderHiddenState, encoderCellState = tf.keras.layers.LSTM(units = lstm_units, return_sequences = True, return_state = True, name = \"lstmEncoderLayer\")(encoderEmbedding)\n",
    "\n",
    "\n",
    "\n",
    "#decoder input, embedding, lstm and dense\n",
    "\n",
    "# input tensor for the decoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputDecoderTensor = tf.keras.Input(shape=(max_length, ), name=\"inputDecoderTensor\")\n",
    "\n",
    "# embedding layer of the decoder, the input is the input tensor, the output is the embedding tensor\n",
    "decoderEmbedding = tf.keras.layers.Embedding(len(word2index) + 1, output_dim = outputDimension, input_length = max_length, trainable = True, name = \"embeddingDecoderLayer\")(inputDecoderTensor)\n",
    "\n",
    "# LSTM layer of the decoder, the input is the embedding tensor and the state of the previous lstm layer, the output is the output tensor and the hidden state of the decoder\n",
    "decoderLSTM, decoderHiddenState, decoderCellState = tf.keras.layers.LSTM(units = lstm_units, return_sequences = True, return_state = True, name = \"lstmDecoderLayer\")(decoderEmbedding, initial_state = [encoderHiddenState, encoderCellState])\n",
    "\n",
    "# dense layer of the decoder, the input is the output tensor of the lstm layer, the output is the output tensor of the dense layer\n",
    "# the dense layer has the same number of units as the number of words in the dictionary because the output of the dense layer is a vector with a probability for each word in the dictionary\n",
    "decoderDense = tf.keras.layers.Dense(units = len(word2index), activation = \"softmax\", name = \"denseLayer\")(decoderLSTM)\n",
    "\n",
    "# Define the model \n",
    "model = tf.keras.models.Model([inputEncoderTensor, inputDecoderTensor], decoderDense)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# generate an early stopping callback to stop training when the validation loss stops improving so that the model doesn't overfit\n",
    "# waits 3 epochs before stopping\n",
    "# use val_loss as the metric because categorical_crossentropy calculates the difference between the predicted and actual values and by monitoring wether or not the loss would be decreasing or increasing we can see if the model is improving or not\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Train the model on the training data and evaluate it on the validation data\n",
    "model.fit(x=[inputEncoder, inputDecoder],y=outputDecoder , epochs=10, batch_size=32, callbacks=[earlyStopping])\n",
    "\n",
    "# Evaluate the model on the validation set and store the results\n",
    "loss, accuracy = model.evaluate([test_inputEncoder, test_inputDecoder], test_outputDecoder)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trainieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    if weights != []:\n",
    "        numpy.savez(f'models/{layer.name}.npz', weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ausführen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_ml_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9978dee63576bf3eb01fd0099124cb1b0d7ef3663923ee832c1b14872d283213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
