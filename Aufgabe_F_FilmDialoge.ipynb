{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe-F-FilmDialoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeines\n",
    "\n",
    "Eine allgemeine Beschreibung der Laboraufgaben inklusive des Vorgehens, den Bewertungsrichtlinien und der Abgabe finden Sie  <a href=\"ML-allgemein.ipynb\">hier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenquelle\n",
    "\n",
    "\n",
    "* Laden Sie ihre Daten von http://141.72.190.207/ml_lab/F_dialoge herunter\n",
    "    * Die Daten sind geschützt. \n",
    "        * Sie müssen evtl. in einem Netzwerk der DHBW (z.B. WLAN, VPN, ...) angemeldet sein. \n",
    "        * Sie können sich auf der Webseite mit dem Benutzernamen dhbw und dem Zugangsnamen \"ml_LaB_4$\" anmelden. \n",
    "* Die Daten sind in einem anwendungsspezifischen Format gespeichert.\n",
    "    * Sie finden evtl. Informationen über die Daten in einer \"README\" Datei. \n",
    "    * Finden Sie keine solche Datei sind die Daten selbst erklärend. \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten-Sammlung \n",
    "* besteht aus Dialogen aus verschiedensten Filmen\n",
    "* ist in der Readme Datei beschrieben\n",
    "\n",
    "Erstellen Sie aus den einen Chatbot, der auf eine Frage mit einer Antwort im \"Filmjargon\" antwortet! \n",
    "* Verwenden Sie tiefe Neuronale Netze zu Erstellen des Chatbots! \n",
    "* Passen Sie den Chatbot so an, dass er für unterschiedliche Film-Genres unterschiedlich antwortet! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lösung\n",
    "\n",
    "Die Lösung der Aufgabe besteht aus mehreren Teilschritten, welche im Folgenden kurz genannt werden:\n",
    "\n",
    "* Daten einlesen\n",
    "* Daten vorverarbeiten\n",
    "* Model erstellen\n",
    "* Model trainieren\n",
    "* Model abspeichern\n",
    "* Model ausführen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abhängigkeiten des Projekts\n",
    "\n",
    "* Tensorflow 2.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random \n",
    "import re #re = regular expressions\n",
    "import os\n",
    "import numpy as np \n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Abhängigkeiten des Projekts\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.data_helper as dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "Hier werden zuerst die rohen Daten eingelesen und innerhalb der Hilfsfunktion \"readDataToLines\" mit des \"newline\" Zeichen getrennt. Die Daten werden zwei Listen gespeichert, welche dann zurückgegeben werden.\n",
    "Verwendet wurden folgende Funktionen:\n",
    "\n",
    "* \"open\" um die Datei zu öffnen\n",
    "* \"read\" um die Datei zu lesen\n",
    "* \"split\" um die Daten anhand des \"newline\" Zeichen zu trennen und in einer Liste zu speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the files\n",
    "movie_lines, movie_conversations = dh.readDataToLines(\"data/unzipped/movie_lines.txt\", \"data/unzipped/movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 221616\n",
      "Länge Antworten: 221616\n"
     ]
    }
   ],
   "source": [
    "# extract and mix conversations to a list\n",
    "conversations_list = dh.readConversationsToList(movie_conversations)\n",
    "# random.shuffle(conversations_list) #todo maybe add this later \n",
    "\n",
    "# create a dictionary that maps each line id to the corresponding line\n",
    "id2line = dh.readLinesToDict(movie_lines)\n",
    "\n",
    "# remove all unnecessary characters from the lines and replace short forms with the full words\n",
    "id2line = dh.cleanLines(id2line)\n",
    "\n",
    "# split the conversations into requests and responses, each answer is used as a request for the next answer\n",
    "requests, responses = dh.splitConversationsToRequestAndResponse(conversations_list, id2line)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")\n",
    "\n",
    "# delete temporary variables\n",
    "del(movie_lines, movie_conversations, conversations_list, id2line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "# get the length of each line of the requests and save the occurrences in a dictionary\n",
    "lengths_request = {}\n",
    "for sentence in requests:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_request:\n",
    "        lengths_request[length] += 1\n",
    "    else:\n",
    "        lengths_request[length] = 1\n",
    "\n",
    "# get the length of each line of the responses and save the occurrences in a dictionary\n",
    "lengths_response = {}\n",
    "for sentence in responses:\n",
    "    length = len(sentence)\n",
    "    if length in lengths_response:\n",
    "        lengths_response[length] += 1\n",
    "    else:\n",
    "        lengths_response[length] = 1\n",
    "\n",
    "# plot the occurrences of the lengths of the requests and responses in the same plot while only showind x values up to 200\n",
    "plt.bar(lengths_request.keys(), lengths_request.values(), label=\"request\")\n",
    "plt.bar(lengths_response.keys(), lengths_response.values(), label=\"response\")\n",
    "plt.xlim(0, 400)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge Fragen: 97310\n",
      "Länge Antworten: 97310\n"
     ]
    }
   ],
   "source": [
    "# based on the result set the parameters\n",
    "max_wordcount_in_sentence = 30\n",
    "\n",
    "# delete requests/responses with length > max_wordcount_in_sentence\n",
    "requests, responses = dh.removeLongSequences(requests, responses, 1, max_wordcount_in_sentence)\n",
    "\n",
    "print(f\"Länge Fragen: {len(requests)}\")\n",
    "print(f\"Länge Antworten: {len(responses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the size of the dataset\n",
    "requests = requests[:num_examples]\n",
    "responses = responses[:num_examples]\n",
    "\n",
    "# encapsule the requests and responses with the tokens <S> (Start) and <E> (End)\n",
    "requests = dh.encapsuleWithTokens(requests, '<S>', '<E>')\n",
    "responses = dh.encapsuleWithTokens(responses, '<S>', '<E>')#todo muss das so?\n",
    "\n",
    "# get a dictionary of all unique words with their frequency\n",
    "word2count = dh.getWord2Count(requests, responses)\n",
    "\n",
    "# filter out words with a certain frequency\n",
    "min_wordFrequency = 1\n",
    "word2count = {k:v for k,v in word2count.items() if v > min_wordFrequency}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man in der obigen Grafik erkennt, sind ab einer Sequenzlänge von 200 nur noch wenige Daten vorhanden. Da die Sequenzlänge maßgeblich auch für den Speicherbedarf in weiteren Schritten ist, wird diese hier auf 200 begrenzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = '<UNK>'\n",
    "maxlen = len(word2count) + 1\n",
    "del(word2count)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_token, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(requests)\n",
    "tokenizer.fit_on_texts(responses)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_requests = tokenizer.texts_to_sequences(requests)\n",
    "tokenized_responses = tokenizer.texts_to_sequences(responses)\n",
    "\n",
    "max_sen_length = dh.get_maximum_sentence_length(tokenized_requests, tokenized_responses)\n",
    "\n",
    "\n",
    "pad_type = 'post'\n",
    "\n",
    "input_encoder = pad_sequences(tokenized_requests, padding=pad_type, maxlen=max_sen_length)\n",
    "input_decoder = pad_sequences(tokenized_responses, padding=pad_type, maxlen=max_sen_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = [i[1:] for i in tokenized_responses]\n",
    "decoder_output = pad_sequences(decoder_output, maxlen=max_sen_length, padding='post')\n",
    "\n",
    "# input_encoder_train, input_encoder_val, input_decoder_train, input_decoder_val = train_test_split(input_tensor, target_tensor, test_size=test_size)\n",
    "# todo?\n",
    "\n",
    "decoder_output = tf.keras.utils.to_categorical(decoder_output, num_classes=vocab_size)\n",
    "\n",
    "# Decoder Output erstellen (removed start, because don't want this to be generated)\n",
    "# output_decoder_train = [i[1:] for i in input_decoder_train]\n",
    "# output_decoder_val = [i[1:] for i in input_decoder_val]\n",
    "\n",
    "#Dimension erhöhen\n",
    "#output_decoder_train = pad_sequences(output_decoder_train, padding=pad_type, maxlen=max_sen_length)\n",
    "#output_decoder_val = pad_sequences(output_decoder_val, padding=pad_type, maxlen=max_sen_length)\n",
    "\n",
    "# convert the decoder input to a one-hot encoded vector which the model can use\n",
    "#output_decoder_train = tf.keras.utils.to_categorical(output_decoder_train, num_classes=vocab_size) # using float32 even though it's memory intensive because later steps need it to be float32\n",
    "#output_decoder_val = tf.keras.utils.to_categorical(output_decoder_val, num_classes=vocab_size, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model erstellen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDimension = 50 \n",
    "lstm_units = 256\n",
    "max_sentence_length = max_sen_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the encoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputEncoderTensor = tf.keras.Input(shape=(None, ), name = \"inputEncoderTensor\")\n",
    "\n",
    "# embedding layer of the encoder, the input is the input tensor, the output is the embedding tensor\n",
    "encoderEmbedding = tf.keras.layers.Embedding(vocab_size, outputDimension, mask_zero=True)(inputEncoderTensor)\n",
    "\n",
    "# LSTM layer of the encoder, the input is the embedding tensor, the output is the output tensor and the hidden state of the encoder\n",
    "encoderLSTM, encoderHiddenState, encoderCellState = tf.keras.layers.LSTM(outputDimension, return_sequences = True, return_state = True)(encoderEmbedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- Input\n",
    "- Embedding\n",
    "- LSTM\n",
    "- Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor for the decoder, shape of each vector is determined by max_length which was also used to pad the data\n",
    "inputDecoderTensor = tf.keras.Input(shape=(None, ))\n",
    "\n",
    "# embedding layer of the decoder, the input is the input tensor, the output is the embedding tensor\n",
    "decoderEmbedding = tf.keras.layers.Embedding(vocab_size, outputDimension, mask_zero=True, trainable = True)(inputDecoderTensor)\n",
    "\n",
    "# LSTM layer of the decoder, the input is the embedding tensor and the state of the previous lstm layer, the output is the output tensor and the hidden state of the decoder\n",
    "decoderLSTM, decoderHiddenState, decoderCellState = tf.keras.layers.LSTM(outputDimension, return_sequences = True, return_state = True)(decoderEmbedding, initial_state = [encoderHiddenState, encoderCellState])\n",
    "\n",
    "# dense layer of the decoder, the input is the output tensor of the lstm layer, the output is the output tensor of the dense layer\n",
    "# the dense layer has the same number of units as the number of words in the dictionary because the output of the dense layer is a vector with a probability for each word in the dictionary\n",
    "decoderDense = tf.keras.layers.Dense(vocab_size, activation = \"softmax\")(decoderLSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputEncoderTensor (InputLayer  [(None, None)]      0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 50)     338150      ['inputEncoderTensor[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 50)     338150      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, None, 50),   20200       ['embedding[0][0]']              \n",
      "                                 (None, 50),                                                      \n",
      "                                 (None, 50)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 50),   20200       ['embedding_1[0][0]',            \n",
      "                                 (None, 50),                      'lstm[0][1]',                   \n",
      "                                 (None, 50)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 6763)   344913      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,061,613\n",
      "Trainable params: 1,061,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model \n",
    "model = tf.keras.models.Model([inputEncoderTensor, inputDecoderTensor], decoderDense)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainieren des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m earlyStopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Train the model on the training data and evaluate it on the validation data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model\u001b[39m.\u001b[39;49mfit([input_encoder, input_decoder], decoder_output, epochs\u001b[39m=\u001b[39;49m\u001b[39m240\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[earlyStopping])\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\ML_Tests\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate an early stopping callback to stop training when the validation loss stops improving so that the model doesn't overfit\n",
    "# waits 3 epochs before stopping\n",
    "# use val_loss as the metric because categorical_crossentropy calculates the difference between the predicted and actual values and by monitoring wether or not the loss would be decreasing or increasing we can see if the model is improving or not\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Train the model on the training data and evaluate it on the validation data\n",
    "model.fit([input_encoder, input_decoder], decoder_output, epochs=20, batch_size=50, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 24s 96ms/step - loss: 1.4551\n",
      "Test loss: 1.4551364183425903\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set and store the results\n",
    "\n",
    "# loss = model.evaluate([input_encoder_val, input_decoder_val], output_decoder_val)\n",
    "# print('Test loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell speichern\n",
    "dh.save_model(model, 'model.h5')\n",
    "\n",
    "# # Gewichte speichern\n",
    "# for layer in model.layers:\n",
    "#     weights = layer.get_weights()\n",
    "#     if weights != []:\n",
    "#         np.savez(f'models/{layer.name}.npz', weights, dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model verwenden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two inputs for the state vectors returned by encoder\n",
    "dec_state_input_h = Input(shape=(outputDimension,))\n",
    "dec_state_input_c = Input(shape=(outputDimension,))\n",
    "dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "# these state vectors are used as an initial state \n",
    "# for LSTM layer in the inference decoder\n",
    "# third input is the Embedding layer as explained above  \n",
    "decoderLSTM = tf.keras.layers.LSTM(outputDimension, return_sequences = True, return_state = True) \n",
    "dec_outputs, state_h, state_c = decoderLSTM(decoderEmbedding,\n",
    "                                initial_state=dec_states_inputs)\n",
    "dec_states = [state_h, state_c]\n",
    "# Dense layer is used to return OHE predicted word\n",
    "dec_outputs = tf.keras.layers.Dense(vocab_size, activation = \"softmax\")(dec_outputs)\n",
    "dec_model = Model(\n",
    "    inputs=[inputDecoderTensor] + dec_states_inputs,\n",
    "    outputs=[dec_outputs] + dec_states)\n",
    "\n",
    "# single encoder input is a question, represented as a sequence \n",
    "# of integers padded with zeros\n",
    "enc_model = Model(inputs=inputEncoderTensor, outputs=[encoderHiddenState, encoderCellState])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence: str):\n",
    "    # convert input string to lowercase, \n",
    "    # then split it by whitespaces\n",
    "    words = sentence.lower().split()\n",
    "    # and then convert to a sequence \n",
    "    # of integers padded with zeros\n",
    "    tokens_list = list()\n",
    "    for current_word in words:\n",
    "        result = tokenizer.word_index.get(current_word, '')\n",
    "        if result != '':\n",
    "            tokens_list.append(result)\n",
    "    return pad_sequences([tokens_list],\n",
    "                         maxlen=max_sen_length,\n",
    "                         padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " crotch crotch\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " crotch crotch crotch allure allure\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " crotch crotch crotch allure allure allure\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure allure\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary tracy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary tracy junkie\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary tracy junkie junkie\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary tracy junkie junkie doubted\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      " crotch crotch crotch allure allure allure allure allure pray pray fabric snuffboxes snuffboxes entity restored restored restored toughens toughens toughens vatican darlin darlin darlin tracy tracy tracy infirmary tracy junkie junkie doubted pianist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encode the input sequence into state vectors\n",
    "states_values = enc_model.predict(\n",
    "    str_to_tokens(\"How are you?\"))\n",
    "# start with a target sequence of size 1 - word 'start'   \n",
    "empty_target_seq = np.zeros((1, 1))\n",
    "empty_target_seq[0, 0] = tokenizer.word_index['<S>']\n",
    "stop_condition = False\n",
    "decoded_translation = ''\n",
    "while not stop_condition:\n",
    "    # feed the state vectors and 1-word target sequence \n",
    "    # to the decoder to produce predictions for the next word\n",
    "    dec_outputs, h, c = dec_model.predict([empty_target_seq] \n",
    "                                            + states_values)                 # sample the next word using these predictions\n",
    "    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "    sampled_word = None\n",
    "    # append the sampled word to the target sequence\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if sampled_word_index == index:\n",
    "            if word != '<E>':\n",
    "                decoded_translation += ' {}'.format(word)\n",
    "            sampled_word = word\n",
    "    # repeat until we generate the end-of-sequence word 'end' \n",
    "    # or we hit the length of answer limit\n",
    "    if sampled_word == '<E>' \\\n",
    "            or len(decoded_translation.split()) \\\n",
    "            > max_sen_length:\n",
    "        stop_condition = True\n",
    "    # prepare next iteration\n",
    "    empty_target_seq = np.zeros((1, 1))\n",
    "    empty_target_seq[0, 0] = sampled_word_index\n",
    "    states_values = [h, c]    \n",
    "    print(decoded_translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9978dee63576bf3eb01fd0099124cb1b0d7ef3663923ee832c1b14872d283213"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
